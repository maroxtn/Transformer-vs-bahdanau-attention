{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzDcfnOvPYnR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "from utils import translate_sentence, save_checkpoint, load_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xMOTOdRgHlAl",
    "outputId": "d9edc366-d533-4859-e2b8-88b2a4090258"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "BkhAPN502QtF",
    "outputId": "aa86a622-f122-4570-94e3-65c04871c452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
      "don't seem to have the spaCy package itself installed (maybe because you've\n",
      "built from source?), so installing the model dependencies would cause spaCy to\n",
      "be downloaded, which probably isn't what you want. If the model package has\n",
      "other dependencies, you'll have to install them manually.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Skipping model package dependencies and setting `--no-deps`. You\n",
      "don't seem to have the spaCy package itself installed (maybe because you've\n",
      "built from source?), so installing the model dependencies would cause spaCy to\n",
      "be downloaded, which probably isn't what you want. If the model package has\n",
      "other dependencies, you'll have to install them manually.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "spacy_de = de_core_news_sm.load()\n",
    "spacy_eng = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "<font size=\"3\"> With the help of spacy tokenizer, the tokenizer for the english and the german language are created. The tokenizer simply splits the individual words of a sentence into an array. A costum tokenizer here was used instead of the python `.split(\" \")` because in some particular a distinct word is not defined by space e.g. punction, possessive s ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xiGiEV4280s"
   },
   "outputs": [],
   "source": [
    "def tokenizer_de(text):\n",
    "  return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenizer_eng(text):\n",
    "  return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cvgTWYkg46Zz",
    "outputId": "f6c81dc4-c3e3-418f-b14d-10a1810a8a2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ate', 'my', 'friends', \"'s\", \"O'neal\", 'apple', 'yesterday']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_eng(\"I ate my friends's O'neal apple yesterday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> A field is a place holder for the dataset that we'll be loading; using it, we specify special tokens (beginning and end tokens). </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-IYtkU05ACP"
   },
   "outputs": [],
   "source": [
    "german = Field(tokenize=tokenizer_de, lower=True, init_token='<sos>', eos_token='<eos>')\n",
    "english = Field(tokenize=tokenizer_eng, lower=True, init_token='<sos>', eos_token='<eos>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">This loads the Multi30k machine translation dataset [link](https://github.com/multi30k/dataset). The return value is three objects containing the train, validation, and test data with the respective size 29000, 1014, 1000. Each object contains two attributes, `.src` contains an array of the original phrase in English and `.trg` contains the  target phrase in German. These words are later to be encoded.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tG2khqwP5RkZ"
   },
   "outputs": [],
   "source": [
    "train_data, validation_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(german, english))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">Build the vocabulary object for both languages. Writing `english.vocab` will this yield this object. It contains a list of frequencies of every word extracted from the original dataset (`english.vocab.freqs`), A dictionary mapping every key to a word, and a dictionary mapping every word to a key (`english.vocab.stoi` and `english.vocab.itos`)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZofP4s77Zkl"
   },
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most common words in the dataset: [('a', 49165), ('.', 27623), ('in', 14886), ('the', 10955), ('on', 8035)]\n"
     ]
    }
   ],
   "source": [
    "#vocab.freqs in a python counter datatype\n",
    "print(\"Five most common words in the dataset: \" + str(english.vocab.freqs.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building the translation model using pytorch**\n",
    "<font size=\"3\"> The Encoder-Decoder model is split into a encoder and a decoder, hence the naming. The decoder maps the input sentence into a representation vector of a predefined length. Intuitively speaking, when a person translates a text, they read the text, understand it, then translate it. We mimic this in a neural networks by representing the meaning in this feature vector. This feature vector is then used by the decoder to turn it into text, but a text into a different language. The implementation of the Encoder-decoder is going to split into three models: Encoder, Decoder, Seq2Seq (which uses the Encoder and the decoder model)</font>\n",
    "\n",
    "<img src=\"encoder.png\" width=\"600\">\n",
    "\n",
    "<font size=\"3\"> The encoder consists of an embedding layer, that converts a sequence of words (The phrase to be translated) into an embedding of shape (vocab_words, embedding_space) then feeds it sequentially to the LSTM layers. Every LSTM layer has both hidden state and cell state, which keep track of what the LSTM unit remembers. We will return that as the output of the Encoder model. </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRksTdJk86iE"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p_dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "\n",
    "        return (hidden, cell)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\"> The decoder part receives the previous word in the sentence as an input (<SOS\\> if it is the first word in the sentence), the LSTM is to be initialized with the cell state and the hidden state of the encoder. The previous word is converted to an embedding, and then fed to the LSTM layer. The returned output is then fed to a dense layer. The output of the dense layer represents the likelihood of each word to be the next word of the translated sentence. Conventionally, we pick the argmax(1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrDzJNnC9BcE"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p_dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.dropout = nn.Dropout(p_dropout)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p_dropout)\n",
    "        self.Fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        output, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "\n",
    "        predictions = self.Fc(output)\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Seq2Seq** is the model that deploys the Encoder and the Decoder model. It takes as input an instances of the encoder and the decoder model. It uses the encoder model to get the latent representation of the input text. After that it loops on the length of the target sequence. In every step of the loop, the decoder is fed the last predicted word, and it inputs the likelihood of every word in the vocabulary of it being the next word. Argmax(1) is used to filter out the word with the heighest likelihood.</font>\n",
    "\n",
    "<font size=\"3\">*Teacher Forcing* is feeding the decoder the actual previous word, not the predicted previous word; since according to the original paper, it is said to help improve the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OYoA3waBGZF"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "  \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J46bfkFBCoOn"
   },
   "outputs": [],
   "source": [
    "#Training Hyperparameters\n",
    "num_epochs = 100\n",
    "lr = 0.001\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUzYsQulCp0X"
   },
   "outputs": [],
   "source": [
    "#Model Hyperparameter\n",
    "load_model = False\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_decoder = len(english.vocab)\n",
    "input_size_encoder = len(german.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLW4Bs-KE-dW"
   },
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, validation_data, test_data), batch_size = batch_size,\n",
    "    sort_within_batch = True,\n",
    "    sort_key = lambda x: len(x.src),\n",
    "    device= device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Md1-WY2sFk_5"
   },
   "outputs": [],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, \n",
    "                      hidden_size, num_layers, enc_dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, \n",
    "                      hidden_size, output_size, num_layers, dec_dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsrP9RA3GQhe"
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TkuWFA8lGXZe"
   },
   "outputs": [],
   "source": [
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JESCqAA7GlBG"
   },
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    load_checkpoint(torch.load('my_checkpoint.pth.ptar'), model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "dN3cxS5oMnsk",
    "outputId": "629bf5b2-d54e-4495-caba-0a0b48a0829b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 100]\n",
      "=> Saving checkpoint\n",
      "torch.Size([17, 1, 300])\n",
      "Translated example sentence: \n",
      " ['couple', 'hovering', 'hovering', 'smock', 'smock', 'smock', 'smock', 'smock', 'nap', '2008', '2008', 'daring', 'daring', 'adult', 'adult', 'lavender', 'lavender', 'hotel', 'lavender', 'wraps', 'saw', 'ponchos', 'ponchos', 'bucked', 'chat', 'chat', 'lavender', 'lavender', 'canoe', 'wraps', 'wraps', 'boarding', 'boarding', 'shown', 'shelves', 'shelves', 'lavender', 'crutches', 'crutches', 'sorting', 'sorting', 'sorting', 'taste', 'taste', 'snorkeling', 'helps', 'helps', 'demonstration', 'cords', 'robed']\n",
      "torch.Size([13, 64, 300])\n",
      "torch.Size([13, 64, 300])\n",
      "torch.Size([11, 64, 300])\n",
      "torch.Size([18, 64, 300])\n",
      "torch.Size([22, 64, 300])\n",
      "torch.Size([13, 64, 300])\n",
      "torch.Size([17, 64, 300])\n",
      "torch.Size([14, 64, 300])\n",
      "torch.Size([12, 64, 300])\n",
      "torch.Size([19, 64, 300])\n",
      "torch.Size([11, 64, 300])\n",
      "torch.Size([10, 64, 300])\n",
      "torch.Size([10, 64, 300])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-f0bccdf253c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentence = \"ein boot mit mehreren männern darauf wird von einem großen pferdegespann ans ufer gezogen.\"\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f'Epoch [{epoch} / {num_epochs}]')\n",
    "\n",
    "    checkpoint = {'state_dict': model.state_dict(), optimizer: optimizer.state_dict()}\n",
    "    save_checkpoint(checkpoint)\n",
    "\n",
    "    model.eval()\n",
    "    translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=50)\n",
    "\n",
    "    print(f\"Translated example sentence: \\n {translated_sentence}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        output = model(inp_data, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CyrheTh3OibP"
   },
   "outputs": [],
   "source": [
    "translate_sentence(model, \"Diese Jungs schauen sich jeden Tag die Nachrichten an\", german, english, device, max_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
