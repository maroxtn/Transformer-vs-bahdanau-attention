{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:13:22.054009Z",
     "iopub.status.busy": "2021-01-27T22:13:22.053343Z",
     "iopub.status.idle": "2021-01-27T22:13:26.408144Z",
     "shell.execute_reply": "2021-01-27T22:13:26.407593Z"
    },
    "papermill": {
     "duration": 4.376272,
     "end_time": "2021-01-27T22:13:26.408265",
     "exception": false,
     "start_time": "2021-01-27T22:13:22.031993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import IWSLT\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import sys \n",
    "import time\n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:13:26.448426Z",
     "iopub.status.busy": "2021-01-27T22:13:26.447794Z",
     "iopub.status.idle": "2021-01-27T22:13:42.752446Z",
     "shell.execute_reply": "2021-01-27T22:13:42.751986Z"
    },
    "papermill": {
     "duration": 16.329802,
     "end_time": "2021-01-27T22:13:42.752561",
     "exception": false,
     "start_time": "2021-01-27T22:13:26.422759",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/maroxtn/IWSLT-BACKUP/archive/main.zip -q\n",
    "!unzip -q main.zip\n",
    "\n",
    "!mkdir .data\n",
    "!mv IWSLT-BACKUP-main/iwslt .data\n",
    "\n",
    "!rm -rf main.zip IWSLT-BACKUP-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:13:42.784796Z",
     "iopub.status.busy": "2021-01-27T22:13:42.784277Z",
     "iopub.status.idle": "2021-01-27T22:13:42.787947Z",
     "shell.execute_reply": "2021-01-27T22:13:42.787509Z"
    },
    "papermill": {
     "duration": 0.021477,
     "end_time": "2021-01-27T22:13:42.788044",
     "exception": false,
     "start_time": "2021-01-27T22:13:42.766567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\"IN_LANG\":\"de\", \"OUT_LANG\": \"en\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:13:42.823655Z",
     "iopub.status.busy": "2021-01-27T22:13:42.822852Z",
     "iopub.status.idle": "2021-01-27T22:14:15.520793Z",
     "shell.execute_reply": "2021-01-27T22:14:15.520289Z"
    },
    "papermill": {
     "duration": 32.718621,
     "end_time": "2021-01-27T22:14:15.520950",
     "exception": false,
     "start_time": "2021-01-27T22:13:42.802329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "if CFG[\"IN_LANG\"] == \"en\":\n",
    "    spacy_in_lang = en_core_web_sm.load()\n",
    "    spacy_out_lang = de_core_news_sm.load()\n",
    "else:\n",
    "    spacy_in_lang = de_core_news_sm.load()\n",
    "    spacy_out_lang = en_core_web_sm.load()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:14:15.605985Z",
     "iopub.status.busy": "2021-01-27T22:14:15.605093Z",
     "iopub.status.idle": "2021-01-27T22:14:15.609896Z",
     "shell.execute_reply": "2021-01-27T22:14:15.609298Z"
    },
    "papermill": {
     "duration": 0.049539,
     "end_time": "2021-01-27T22:14:15.609990",
     "exception": false,
     "start_time": "2021-01-27T22:14:15.560451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenizer_in(text):\n",
    "    return [tok.text for tok in spacy_in_lang.tokenizer(text)]\n",
    "\n",
    "def tokenizer_out(text):\n",
    "    return [tok.text for tok in spacy_out_lang.tokenizer(text)]\n",
    "\n",
    "\n",
    "in_lang = Field(tokenize=tokenizer_in, lower=True, include_lengths=True)\n",
    "out_lang = Field(tokenize=tokenizer_out, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\", include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:14:15.692487Z",
     "iopub.status.busy": "2021-01-27T22:14:15.691981Z",
     "iopub.status.idle": "2021-01-27T22:15:30.901046Z",
     "shell.execute_reply": "2021-01-27T22:15:30.900016Z"
    },
    "papermill": {
     "duration": 75.252751,
     "end_time": "2021-01-27T22:15:30.901186",
     "exception": false,
     "start_time": "2021-01-27T22:14:15.648435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "\n",
    "train_data, valid_data, test_data = IWSLT.splits(root=\"../data\",\n",
    "        exts=(\".\"+CFG[\"IN_LANG\"], \".\"+CFG[\"OUT_LANG\"]), fields=(in_lang, out_lang ),filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:31.024204Z",
     "iopub.status.busy": "2021-01-27T22:15:31.013984Z",
     "iopub.status.idle": "2021-01-27T22:15:32.548915Z",
     "shell.execute_reply": "2021-01-27T22:15:32.547981Z"
    },
    "papermill": {
     "duration": 1.610587,
     "end_time": "2021-01-27T22:15:32.549029",
     "exception": false,
     "start_time": "2021-01-27T22:15:30.938442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_lang.build_vocab(train_data, min_freq=2)\n",
    "out_lang.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:32.633934Z",
     "iopub.status.busy": "2021-01-27T22:15:32.633258Z",
     "iopub.status.idle": "2021-01-27T22:15:32.636460Z",
     "shell.execute_reply": "2021-01-27T22:15:32.636040Z"
    },
    "papermill": {
     "duration": 0.05109,
     "end_time": "2021-01-27T22:15:32.636552",
     "exception": false,
     "start_time": "2021-01-27T22:15:32.585462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=True)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, inp_length=None):\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        if inp_length == None:\n",
    "            encoder_states, hidden = self.rnn(embedding)\n",
    "        else:      \n",
    "            packed = pack_padded_sequence(embedding, inp_length.cpu()) #To speed up training\n",
    "            encoder_states, hidden = self.rnn(packed)\n",
    "            encoder_states, _ = pad_packed_sequence(encoder_states)\n",
    "\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "\n",
    "        return encoder_states, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:32.759640Z",
     "iopub.status.busy": "2021-01-27T22:15:32.758464Z",
     "iopub.status.idle": "2021-01-27T22:15:32.761497Z",
     "shell.execute_reply": "2021-01-27T22:15:32.761068Z"
    },
    "papermill": {
     "duration": 0.088169,
     "end_time": "2021-01-27T22:15:32.761582",
     "exception": false,
     "start_time": "2021-01-27T22:15:32.673413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.GRU(hidden_size * 2 + embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        self.energy = nn.Linear(hidden_size, 1)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.fc_key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_query = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "    def forward(self, x, encoder_states, hidden, source, inp_mask):\n",
    "        \n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        \n",
    "        key = self.fc_key(hidden)\n",
    "        query = self.fc_query(encoder_states)\n",
    "        \n",
    "        energy = key+query\n",
    "        energy = self.energy(torch.tanh(energy))\n",
    "        \n",
    "        if inp_mask != None:\n",
    "            energy = energy.squeeze(-1).masked_fill_(inp_mask, -float('inf')).unsqueeze(-1)\n",
    "\n",
    "        attention = F.softmax(energy, dim=0) #(seq_len, batch, 1)\n",
    "                                             #(seq_len, batch, hidden*2)\n",
    "        \n",
    "        context_vector = torch.bmm(attention.permute(1, 2, 0), encoder_states.permute(1, 0, 2)).permute(1,0,2)\n",
    "\n",
    "        #Concatenate the context vector with the embedding of the previous word, and feed it to the GRU\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        outputs, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "\n",
    "        return predictions, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:32.844463Z",
     "iopub.status.busy": "2021-01-27T22:15:32.843787Z",
     "iopub.status.idle": "2021-01-27T22:15:32.846824Z",
     "shell.execute_reply": "2021-01-27T22:15:32.847213Z"
    },
    "papermill": {
     "duration": 0.049126,
     "end_time": "2021-01-27T22:15:32.847316",
     "exception": false,
     "start_time": "2021-01-27T22:15:32.798190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, inp_length, inp_mask):\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(out_lang.vocab)\n",
    "\n",
    "        \n",
    "        outputs = []\n",
    "        \n",
    "        encoder_states, hidden = self.encoder(source, inp_length)\n",
    "        \n",
    "        x = target[0] #<SOS>\n",
    "        \n",
    "        for t in range(1, target_len):\n",
    "\n",
    "            output, hidden = self.decoder(x, encoder_states, hidden, source, inp_mask)\n",
    "\n",
    "            outputs.append(output)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] #No teacher forcing\n",
    "            \n",
    "        \n",
    "        outputs = torch.cat(outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:32.925468Z",
     "iopub.status.busy": "2021-01-27T22:15:32.923916Z",
     "iopub.status.idle": "2021-01-27T22:15:32.926138Z",
     "shell.execute_reply": "2021-01-27T22:15:32.926542Z"
    },
    "papermill": {
     "duration": 0.042928,
     "end_time": "2021-01-27T22:15:32.926638",
     "exception": false,
     "start_time": "2021-01-27T22:15:32.883710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebatch(batch):\n",
    "    return Batch(batch.src, batch.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:33.008318Z",
     "iopub.status.busy": "2021-01-27T22:15:33.007503Z",
     "iopub.status.idle": "2021-01-27T22:15:33.012141Z",
     "shell.execute_reply": "2021-01-27T22:15:33.011603Z"
    },
    "papermill": {
     "duration": 0.049116,
     "end_time": "2021-01-27T22:15:33.012225",
     "exception": false,
     "start_time": "2021-01-27T22:15:32.963109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "\n",
    "    def __init__(self, src, trg):\n",
    "        \n",
    "        src, src_lengths = src\n",
    "        \n",
    "        self.src = src\n",
    "        self.src_lengths = src_lengths\n",
    "        self.src_mask = (src == in_pad_idx)\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.trg_lengths = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            trg, trg_lengths = trg\n",
    "            self.trg = trg\n",
    "            self.trg_lengths = trg_lengths\n",
    "            self.trg_y = trg[1:].reshape(-1)\n",
    "            self.ntokens = (self.trg_y != out_pad_idx).sum().item()  \n",
    "        \n",
    "        if device == torch.device('cuda'):\n",
    "            self.src = self.src.cuda()\n",
    "            self.src_mask = self.src_mask.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:33.440709Z",
     "iopub.status.busy": "2021-01-27T22:15:33.439828Z",
     "iopub.status.idle": "2021-01-27T22:15:33.442919Z",
     "shell.execute_reply": "2021-01-27T22:15:33.442183Z"
    },
    "papermill": {
     "duration": 0.394396,
     "end_time": "2021-01-27T22:15:33.443024",
     "exception": false,
     "start_time": "2021-01-27T22:15:33.048628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Training Hyperparameters\n",
    "num_epochs = 30\n",
    "lr = 0.0003\n",
    "batch_size = 64\n",
    "d_model = 256\n",
    "\n",
    "input_size_encoder = len(in_lang.vocab)\n",
    "input_size_decoder = len(out_lang.vocab)\n",
    "output_size = len(out_lang.vocab)\n",
    "\n",
    "\n",
    "encoder_embedding_size = d_model\n",
    "decoder_embedding_size = d_model\n",
    "hidden_size = d_model\n",
    "\n",
    "num_layers = 1\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:33.523449Z",
     "iopub.status.busy": "2021-01-27T22:15:33.522412Z",
     "iopub.status.idle": "2021-01-27T22:15:33.524429Z",
     "shell.execute_reply": "2021-01-27T22:15:33.524905Z"
    },
    "papermill": {
     "duration": 0.044533,
     "end_time": "2021-01-27T22:15:33.525012",
     "exception": false,
     "start_time": "2021-01-27T22:15:33.480479",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:33.603924Z",
     "iopub.status.busy": "2021-01-27T22:15:33.603280Z",
     "iopub.status.idle": "2021-01-27T22:15:38.802825Z",
     "shell.execute_reply": "2021-01-27T22:15:38.801796Z"
    },
    "papermill": {
     "duration": 5.24121,
     "end_time": "2021-01-27T22:15:38.802963",
     "exception": false,
     "start_time": "2021-01-27T22:15:33.561753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, \n",
    "                      hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, \n",
    "                      hidden_size, output_size, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:38.887751Z",
     "iopub.status.busy": "2021-01-27T22:15:38.887145Z",
     "iopub.status.idle": "2021-01-27T22:15:38.890069Z",
     "shell.execute_reply": "2021-01-27T22:15:38.890474Z"
    },
    "papermill": {
     "duration": 0.049744,
     "end_time": "2021-01-27T22:15:38.890578",
     "exception": false,
     "start_time": "2021-01-27T22:15:38.840834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23195573"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:38.972335Z",
     "iopub.status.busy": "2021-01-27T22:15:38.971614Z",
     "iopub.status.idle": "2021-01-27T22:15:38.974774Z",
     "shell.execute_reply": "2021-01-27T22:15:38.974347Z"
    },
    "papermill": {
     "duration": 0.046471,
     "end_time": "2021-01-27T22:15:38.974883",
     "exception": false,
     "start_time": "2021-01-27T22:15:38.928412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_pad_idx = in_lang.vocab.stoi['<pad>']\n",
    "out_pad_idx = out_lang.vocab.stoi['<pad>']\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=out_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:39.060591Z",
     "iopub.status.busy": "2021-01-27T22:15:39.057583Z",
     "iopub.status.idle": "2021-01-27T22:15:39.063132Z",
     "shell.execute_reply": "2021-01-27T22:15:39.062635Z"
    },
    "papermill": {
     "duration": 0.050522,
     "end_time": "2021-01-27T22:15:39.063216",
     "exception": false,
     "start_time": "2021-01-27T22:15:39.012694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_sentence_bahdanau(model, sentence, max_length=50):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    tokens = [token.text.lower() for token in spacy_in_lang(sentence)]\n",
    "\n",
    "    text_to_indices = [in_lang.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    preds = [out_lang.vocab.stoi[out_lang.init_token]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        encoder_states, hidden = model.encoder(sentence_tensor)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "                    \n",
    "            trg = torch.Tensor([preds[-1]]).long().to(device)\n",
    "\n",
    "            output, hidden = model.decoder(trg, encoder_states, hidden, sentence_tensor, None)\n",
    "            new = output.argmax(1).item()\n",
    "            \n",
    "            preds.append(new)\n",
    "            \n",
    "            if new == out_lang.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "        \n",
    "    return \" \".join([out_lang.vocab.itos[i] for i in preds][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:39.159670Z",
     "iopub.status.busy": "2021-01-27T22:15:39.151093Z",
     "iopub.status.idle": "2021-01-27T22:15:39.162502Z",
     "shell.execute_reply": "2021-01-27T22:15:39.162098Z"
    },
    "papermill": {
     "duration": 0.061832,
     "end_time": "2021-01-27T22:15:39.162586",
     "exception": false,
     "start_time": "2021-01-27T22:15:39.100754",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beam(phrase, k):  #K: beam width\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sos = out_lang.vocab.stoi[\"<sos>\"]\n",
    "    tgt = [sos]\n",
    "    \n",
    "    #Prepare sentence\n",
    "    tokens = [token.text.lower() for token in spacy_in_lang(phrase)]\n",
    "    tokens.append(in_lang.eos_token)\n",
    "    tokens.insert(0, in_lang.init_token)\n",
    "\n",
    "    text_to_indices = [in_lang.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #Get encoder output\n",
    "        encoder_states, hidden = model.encoder(sentence_tensor)\n",
    "        \n",
    "        \n",
    "        #Get first output from model\n",
    "        trg = torch.Tensor([tgt[-1]]).long().to(device)\n",
    "\n",
    "        output, hidden = model.decoder(trg, encoder_states, hidden,sentence_tensor)\n",
    "        out = F.softmax(output).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        args = out.argsort()[-k:]\n",
    "        probs = out[args].detach().cpu().numpy()\n",
    "        \n",
    "        args = args.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        probs = np.log(probs)\n",
    "        possible = list(zip([tgt + [args[i]] for i in range(k)], probs, [hidden.clone() for j in range(k)]))\n",
    "\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            test=  []\n",
    "            for j in range(k):\n",
    "\n",
    "                tmp_tgt, tmp_prob, tmp_hidden = possible[j]\n",
    "\n",
    "                if tmp_tgt[-1] == out_lang.vocab.stoi[\"<eos>\"]:  #If sentence already ended\n",
    "                    test.append(possible[j])\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    #Compute output\n",
    "                    trg = torch.Tensor([tmp_tgt[-1]]).long().to(device)\n",
    "\n",
    "                    output, hidden = model.decoder(trg, encoder_states, tmp_hidden, sentence_tensor)\n",
    "                    out = F.softmax(output).squeeze()\n",
    "                    \n",
    "                    \n",
    "                    tmp_args = out.argsort()[-k:]\n",
    "                    tmp_probs = out[args].detach().cpu().numpy()\n",
    "\n",
    "                    tmp_args = tmp_args.detach().cpu().numpy()\n",
    "                    tmp_probs = (tmp_prob + np.log(tmp_probs))/(len(tmp_tgt)-1)\n",
    "\n",
    "\n",
    "                    for r in range(k): \n",
    "                        test.append((tmp_tgt + [tmp_args[r]], tmp_probs[r], hidden))\n",
    "\n",
    "\n",
    "            possible = sorted(test, key=lambda x:x[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "                    \n",
    "    \n",
    "    return possible\n",
    "\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    \n",
    "    sentence = x[0]\n",
    "    sentence = [out_lang.vocab.itos[i] for i in sentence]\n",
    "    \n",
    "    return (\" \".join(sentence), x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:39.249541Z",
     "iopub.status.busy": "2021-01-27T22:15:39.248759Z",
     "iopub.status.idle": "2021-01-27T22:15:39.251082Z",
     "shell.execute_reply": "2021-01-27T22:15:39.251496Z"
    },
    "papermill": {
     "duration": 0.050264,
     "end_time": "2021-01-27T22:15:39.251625",
     "exception": false,
     "start_time": "2021-01-27T22:15:39.201361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_epoch(iterator, log_every=100):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    n_tokens = 0\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(iterator):\n",
    "        \n",
    "        inp_data = batch.src\n",
    "        inp_length = batch.src_lengths\n",
    "        inp_mask = batch.src_mask\n",
    "        \n",
    "        target = batch.trg\n",
    "\n",
    "        output = model(inp_data, target, inp_length, inp_mask)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(output, batch.trg_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        n_tokens += batch.ntokens\n",
    "        \n",
    "        \n",
    "        \n",
    "        if (batch_idx % log_every == 0) and (batch_idx > 0):\n",
    "            tokens_per_sec = n_tokens/(time.time() - start)\n",
    "            print(\" Step %d - Loss %f - Tokens per Sec %f\" % (batch_idx, loss.item(), tokens_per_sec))\n",
    "        \n",
    "    return total_loss / len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:39.334212Z",
     "iopub.status.busy": "2021-01-27T22:15:39.333521Z",
     "iopub.status.idle": "2021-01-27T22:15:39.336516Z",
     "shell.execute_reply": "2021-01-27T22:15:39.336024Z"
    },
    "papermill": {
     "duration": 0.047169,
     "end_time": "2021-01-27T22:15:39.336595",
     "exception": false,
     "start_time": "2021-01-27T22:15:39.289426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_validation(iterator):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(iterator):\n",
    "        \n",
    "        inp_data = batch.src\n",
    "        inp_length = batch.src_lengths\n",
    "        inp_mask = batch.src_mask\n",
    "        \n",
    "        target = batch.trg\n",
    "\n",
    "        output = model(inp_data, target, inp_length, inp_mask)\n",
    "        \n",
    "        loss = criterion(output, batch.trg_y)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T22:15:39.469401Z",
     "iopub.status.busy": "2021-01-27T22:15:39.454013Z",
     "iopub.status.idle": "2021-01-27T23:27:47.531543Z",
     "shell.execute_reply": "2021-01-27T23:27:47.530511Z"
    },
    "papermill": {
     "duration": 4328.157184,
     "end_time": "2021-01-27T23:27:47.531657",
     "exception": false,
     "start_time": "2021-01-27T22:15:39.374473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 100]\n",
      "\n",
      " Step 100 - Loss 5.982434 - Tokens per Sec 13758.775884\n",
      " Step 200 - Loss 3.893672 - Tokens per Sec 14209.725900\n",
      " Step 300 - Loss 4.406250 - Tokens per Sec 14478.539284\n",
      " Step 400 - Loss 5.292997 - Tokens per Sec 14530.438233\n",
      " Step 500 - Loss 4.351924 - Tokens per Sec 14466.739310\n",
      " Step 600 - Loss 5.011229 - Tokens per Sec 14463.826019\n",
      " Step 700 - Loss 4.707049 - Tokens per Sec 14511.547157\n",
      " Step 800 - Loss 4.504529 - Tokens per Sec 14592.792488\n",
      " Step 900 - Loss 5.019985 - Tokens per Sec 14607.787666\n",
      " Step 1000 - Loss 4.487456 - Tokens per Sec 14613.575630\n",
      " Step 1100 - Loss 4.043857 - Tokens per Sec 14630.879244\n",
      " Step 1200 - Loss 3.763046 - Tokens per Sec 14673.933677\n",
      " Step 1300 - Loss 3.820462 - Tokens per Sec 14662.261897\n",
      " Step 1400 - Loss 3.974252 - Tokens per Sec 14666.482315\n",
      " Step 1500 - Loss 4.023894 - Tokens per Sec 14662.590000\n",
      " Step 1600 - Loss 4.462840 - Tokens per Sec 14668.570615\n",
      " Step 1700 - Loss 3.377359 - Tokens per Sec 14685.289045\n",
      " Step 1800 - Loss 4.651781 - Tokens per Sec 14690.607652\n",
      " Step 1900 - Loss 3.692136 - Tokens per Sec 14701.417872\n",
      " Step 2000 - Loss 4.067434 - Tokens per Sec 14684.150910\n",
      " Step 2100 - Loss 3.934605 - Tokens per Sec 14679.999423\n",
      " Step 2200 - Loss 4.102574 - Tokens per Sec 14702.751245\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and so the first thing , and then , and then .\n",
      "Expected: and the third of these big changes : accountability .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: if you 're not a good , you 're not a lot of it .\n",
      "Expected: if you ai n't a gardener , you ai n't gangsta .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: but now , you have to do it about it .\n",
      "Expected: but now you do n't have to worry about it .\n",
      "\n",
      "\n",
      " Train loss 4.505464088879476 | Validation loss 3.697806553407149 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [1 / 100]\n",
      "\n",
      " Step 100 - Loss 4.048151 - Tokens per Sec 14234.861349\n",
      " Step 200 - Loss 3.816071 - Tokens per Sec 14072.446592\n",
      " Step 300 - Loss 3.791160 - Tokens per Sec 14324.838666\n",
      " Step 400 - Loss 2.258298 - Tokens per Sec 14371.658711\n",
      " Step 500 - Loss 4.249597 - Tokens per Sec 14491.731062\n",
      " Step 600 - Loss 3.618142 - Tokens per Sec 14515.174232\n",
      " Step 700 - Loss 3.850914 - Tokens per Sec 14432.702924\n",
      " Step 800 - Loss 3.410013 - Tokens per Sec 14505.862013\n",
      " Step 900 - Loss 3.893849 - Tokens per Sec 14505.193944\n",
      " Step 1000 - Loss 3.066499 - Tokens per Sec 14580.309543\n",
      " Step 1100 - Loss 3.477810 - Tokens per Sec 14584.627859\n",
      " Step 1200 - Loss 3.728166 - Tokens per Sec 14563.722274\n",
      " Step 1300 - Loss 3.689923 - Tokens per Sec 14540.097677\n",
      " Step 1400 - Loss 3.682070 - Tokens per Sec 14562.724248\n",
      " Step 1500 - Loss 3.153283 - Tokens per Sec 14591.515015\n",
      " Step 1600 - Loss 3.168957 - Tokens per Sec 14604.862135\n",
      " Step 1700 - Loss 3.686573 - Tokens per Sec 14593.419939\n",
      " Step 1800 - Loss 3.990028 - Tokens per Sec 14607.313505\n",
      " Step 1900 - Loss 3.639765 - Tokens per Sec 14600.863085\n",
      " Step 2000 - Loss 3.935953 - Tokens per Sec 14622.597885\n",
      " Step 2100 - Loss 3.392650 - Tokens per Sec 14623.082014\n",
      " Step 2200 - Loss 3.817784 - Tokens per Sec 14619.992214\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: rsw : what ? i do ?\n",
      "Expected: woody : what ? am i hearing correctly ?\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: they would be able to be how to be able to be .\n",
      "Expected: you 'd be surprised how kids are affected by this .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: he was not to the <unk> - old - time . he was n't even him .\n",
      "Expected: he was not invited to the screening . he was too young for that .\n",
      "\n",
      "\n",
      " Train loss 3.6264438486333717 | Validation loss 3.324591354890303 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [2 / 100]\n",
      "\n",
      " Step 100 - Loss 3.257196 - Tokens per Sec 14529.742485\n",
      " Step 200 - Loss 3.263984 - Tokens per Sec 14625.792507\n",
      " Step 300 - Loss 3.819466 - Tokens per Sec 14804.682354\n",
      " Step 400 - Loss 3.543983 - Tokens per Sec 14724.064778\n",
      " Step 500 - Loss 3.466534 - Tokens per Sec 14695.199934\n",
      " Step 600 - Loss 2.862374 - Tokens per Sec 14696.061423\n",
      " Step 700 - Loss 3.695654 - Tokens per Sec 14687.903941\n",
      " Step 800 - Loss 3.581385 - Tokens per Sec 14725.443561\n",
      " Step 900 - Loss 3.918250 - Tokens per Sec 14710.537695\n",
      " Step 1000 - Loss 3.225573 - Tokens per Sec 14674.183464\n",
      " Step 1100 - Loss 3.711220 - Tokens per Sec 14663.117105\n",
      " Step 1200 - Loss 3.969054 - Tokens per Sec 14658.198306\n",
      " Step 1300 - Loss 3.140588 - Tokens per Sec 14679.929924\n",
      " Step 1400 - Loss 2.941789 - Tokens per Sec 14673.411399\n",
      " Step 1500 - Loss 3.771813 - Tokens per Sec 14663.783533\n",
      " Step 1600 - Loss 2.913257 - Tokens per Sec 14671.877641\n",
      " Step 1700 - Loss 3.725447 - Tokens per Sec 14700.837256\n",
      " Step 1800 - Loss 3.395777 - Tokens per Sec 14694.119777\n",
      " Step 1900 - Loss 1.137221 - Tokens per Sec 14690.457760\n",
      " Step 2000 - Loss 3.515834 - Tokens per Sec 14683.371209\n",
      " Step 2100 - Loss 3.524629 - Tokens per Sec 14686.153645\n",
      " Step 2200 - Loss 2.540087 - Tokens per Sec 14699.942554\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: the <unk> is not what the solution is .\n",
      "Expected: the inventors do n't know what the invention is .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: so here 's our own .\n",
      "Expected: so this is our reconstruction .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: and then you can see through this book and then , and you 're looking at the end and all the same time , all of the <unk> .\n",
      "Expected: then you can flip through this book while highlighting the lines , words on the virtual touch pad below each floating window .\n",
      "\n",
      "\n",
      " Train loss 3.268001566255098 | Validation loss 3.129729682748968 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [3 / 100]\n",
      "\n",
      " Step 100 - Loss 2.763117 - Tokens per Sec 14366.856812\n",
      " Step 200 - Loss 3.548115 - Tokens per Sec 14579.122477\n",
      " Step 300 - Loss 2.642173 - Tokens per Sec 14593.700678\n",
      " Step 400 - Loss 2.463204 - Tokens per Sec 14649.310009\n",
      " Step 500 - Loss 2.664834 - Tokens per Sec 14669.598318\n",
      " Step 600 - Loss 3.017999 - Tokens per Sec 14731.282285\n",
      " Step 700 - Loss 3.751800 - Tokens per Sec 14720.587731\n",
      " Step 800 - Loss 2.347526 - Tokens per Sec 14685.410465\n",
      " Step 900 - Loss 2.605774 - Tokens per Sec 14678.479373\n",
      " Step 1000 - Loss 3.446668 - Tokens per Sec 14726.715287\n",
      " Step 1100 - Loss 3.266769 - Tokens per Sec 14722.121623\n",
      " Step 1200 - Loss 3.079711 - Tokens per Sec 14729.359608\n",
      " Step 1300 - Loss 2.567429 - Tokens per Sec 14759.129215\n",
      " Step 1400 - Loss 1.759945 - Tokens per Sec 14707.748162\n",
      " Step 1500 - Loss 3.516220 - Tokens per Sec 14725.700112\n",
      " Step 1600 - Loss 3.180254 - Tokens per Sec 14707.826740\n",
      " Step 1700 - Loss 2.822335 - Tokens per Sec 14717.514598\n",
      " Step 1800 - Loss 3.496170 - Tokens per Sec 14721.727562\n",
      " Step 1900 - Loss 3.154445 - Tokens per Sec 14682.323058\n",
      " Step 2000 - Loss 2.719126 - Tokens per Sec 14688.082501\n",
      " Step 2100 - Loss 2.877145 - Tokens per Sec 14690.397064\n",
      " Step 2200 - Loss 2.681757 - Tokens per Sec 14704.164932\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: because it 's not just a good thing .\n",
      "Expected: because it 's neither rare nor well done .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: thank you .\n",
      "Expected: thank you .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: then i 'm going to show this guy , you 've got to be the <unk> of women .\n",
      "Expected: and then the inspiration came from these beautiful buttons of the ethiopian women 's dresses .\n",
      "\n",
      "\n",
      " Train loss 3.0073506905129834 | Validation loss 2.976884798570113 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [4 / 100]\n",
      "\n",
      " Step 100 - Loss 3.261573 - Tokens per Sec 14581.926170\n",
      " Step 200 - Loss 2.701437 - Tokens per Sec 14058.996090\n",
      " Step 300 - Loss 2.719377 - Tokens per Sec 14258.521866\n",
      " Step 400 - Loss 2.514438 - Tokens per Sec 14359.425622\n",
      " Step 500 - Loss 2.848735 - Tokens per Sec 14393.770451\n",
      " Step 600 - Loss 3.155817 - Tokens per Sec 14515.356849\n",
      " Step 700 - Loss 3.244953 - Tokens per Sec 14489.896901\n",
      " Step 800 - Loss 2.545978 - Tokens per Sec 14549.196169\n",
      " Step 900 - Loss 2.201940 - Tokens per Sec 14581.461042\n",
      " Step 1000 - Loss 2.937599 - Tokens per Sec 14609.013278\n",
      " Step 1100 - Loss 2.433571 - Tokens per Sec 14624.356821\n",
      " Step 1200 - Loss 2.461001 - Tokens per Sec 14592.775660\n",
      " Step 1300 - Loss 2.564815 - Tokens per Sec 14610.213935\n",
      " Step 1400 - Loss 3.299416 - Tokens per Sec 14624.129692\n",
      " Step 1500 - Loss 2.840934 - Tokens per Sec 14636.773661\n",
      " Step 1600 - Loss 3.194619 - Tokens per Sec 14636.591702\n",
      " Step 1700 - Loss 3.222199 - Tokens per Sec 14609.958814\n",
      " Step 1800 - Loss 3.048239 - Tokens per Sec 14639.829158\n",
      " Step 1900 - Loss 2.795135 - Tokens per Sec 14644.871004\n",
      " Step 2000 - Loss 2.734611 - Tokens per Sec 14653.839313\n",
      " Step 2100 - Loss 2.445716 - Tokens per Sec 14652.486052\n",
      " Step 2200 - Loss 2.998777 - Tokens per Sec 14636.034188\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: \" <unk> , <unk> <unk> . \"\n",
      "Expected: \" gouverner , c'est prévoir . \"\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i was so shocked .\n",
      "Expected: i was so shocked .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: that led me to use the same thing .\n",
      "Expected: this is really what brought me to using satellite imagery .\n",
      "\n",
      "\n",
      " Train loss 2.8003061722845812 | Validation loss 2.8691222450949927 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [5 / 100]\n",
      "\n",
      " Step 100 - Loss 2.943275 - Tokens per Sec 14427.632490\n",
      " Step 200 - Loss 2.569011 - Tokens per Sec 14581.320176\n",
      " Step 300 - Loss 2.474763 - Tokens per Sec 14531.662401\n",
      " Step 400 - Loss 3.021324 - Tokens per Sec 14481.097668\n",
      " Step 500 - Loss 3.011814 - Tokens per Sec 14505.057757\n",
      " Step 600 - Loss 3.100194 - Tokens per Sec 14545.264958\n",
      " Step 700 - Loss 3.155596 - Tokens per Sec 14555.626215\n",
      " Step 800 - Loss 3.003942 - Tokens per Sec 14623.646981\n",
      " Step 900 - Loss 2.687202 - Tokens per Sec 14564.116481\n",
      " Step 1000 - Loss 2.896924 - Tokens per Sec 14559.174669\n",
      " Step 1100 - Loss 2.774858 - Tokens per Sec 14581.854512\n",
      " Step 1200 - Loss 2.376066 - Tokens per Sec 14588.001593\n",
      " Step 1300 - Loss 1.829610 - Tokens per Sec 14619.868439\n",
      " Step 1400 - Loss 1.827885 - Tokens per Sec 14591.019474\n",
      " Step 1500 - Loss 2.814923 - Tokens per Sec 14602.177644\n",
      " Step 1600 - Loss 2.534378 - Tokens per Sec 14627.443485\n",
      " Step 1700 - Loss 2.491908 - Tokens per Sec 14622.672672\n",
      " Step 1800 - Loss 2.420358 - Tokens per Sec 14639.551722\n",
      " Step 1900 - Loss 2.972836 - Tokens per Sec 14610.466445\n",
      " Step 2000 - Loss 1.855164 - Tokens per Sec 14622.607522\n",
      " Step 2100 - Loss 2.316085 - Tokens per Sec 14618.337504\n",
      " Step 2200 - Loss 3.388749 - Tokens per Sec 14624.108391\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: now we 're going to use them to the <unk> of the way that we 've been doing before .\n",
      "Expected: and we trick them to respond to pulses of light exactly like we had said before .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: other people can make things , but how would it be with a new universe for a long - dimensional environment , for example .\n",
      "Expected: some other heroes can become invisible , but what about a new power for a future superhero : to see around corners ?\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: thank you .\n",
      "Expected: thank you .\n",
      "\n",
      "\n",
      " Train loss 2.6279263447638757 | Validation loss 2.808419335972179 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [6 / 100]\n",
      "\n",
      " Step 100 - Loss 2.579172 - Tokens per Sec 14299.446001\n",
      " Step 200 - Loss 2.095589 - Tokens per Sec 14266.639820\n",
      " Step 300 - Loss 2.698813 - Tokens per Sec 14352.590701\n",
      " Step 400 - Loss 2.689934 - Tokens per Sec 14482.556096\n",
      " Step 500 - Loss 3.101195 - Tokens per Sec 14481.748701\n",
      " Step 600 - Loss 1.816985 - Tokens per Sec 14581.624705\n",
      " Step 700 - Loss 2.434897 - Tokens per Sec 14517.225170\n",
      " Step 800 - Loss 1.962260 - Tokens per Sec 14536.788263\n",
      " Step 900 - Loss 2.979435 - Tokens per Sec 14576.956110\n",
      " Step 1000 - Loss 2.396904 - Tokens per Sec 14574.602284\n",
      " Step 1100 - Loss 3.010388 - Tokens per Sec 14597.155774\n",
      " Step 1200 - Loss 2.327389 - Tokens per Sec 14506.257249\n",
      " Step 1300 - Loss 2.347565 - Tokens per Sec 14501.490092\n",
      " Step 1400 - Loss 2.801010 - Tokens per Sec 14530.583378\n",
      " Step 1500 - Loss 2.832227 - Tokens per Sec 14547.260791\n",
      " Step 1600 - Loss 1.804988 - Tokens per Sec 14573.104244\n",
      " Step 1700 - Loss 2.135634 - Tokens per Sec 14528.744718\n",
      " Step 1800 - Loss 2.968651 - Tokens per Sec 14552.625202\n",
      " Step 1900 - Loss 2.403334 - Tokens per Sec 14539.772788\n",
      " Step 2000 - Loss 3.013789 - Tokens per Sec 14548.077175\n",
      " Step 2100 - Loss 2.934677 - Tokens per Sec 14558.306979\n",
      " Step 2200 - Loss 0.939043 - Tokens per Sec 14531.254105\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: i think it 's important because the media , all the media , to think , a third way back into our world .\n",
      "Expected: i think this is important because media , all media , provide us a window back into our world .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: one of them was these grandmother .\n",
      "Expected: one of them was this grandmother .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: in the world , i 'm going to be a big deal with my apartment .\n",
      "Expected: suddenly , my disability on the world wide web is fair game .\n",
      "\n",
      "\n",
      " Train loss 2.4819610843607287 | Validation loss 2.7692080519416113 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [7 / 100]\n",
      "\n",
      " Step 100 - Loss 2.325393 - Tokens per Sec 14067.307247\n",
      " Step 200 - Loss 2.334243 - Tokens per Sec 14390.797431\n",
      " Step 300 - Loss 2.315765 - Tokens per Sec 14429.821946\n",
      " Step 400 - Loss 1.644577 - Tokens per Sec 14438.748258\n",
      " Step 500 - Loss 2.463708 - Tokens per Sec 14486.830818\n",
      " Step 600 - Loss 2.963050 - Tokens per Sec 14528.047165\n",
      " Step 700 - Loss 2.009967 - Tokens per Sec 14581.539771\n",
      " Step 800 - Loss 2.295848 - Tokens per Sec 14572.159869\n",
      " Step 900 - Loss 2.549360 - Tokens per Sec 14558.705924\n",
      " Step 1000 - Loss 2.349195 - Tokens per Sec 14566.989249\n",
      " Step 1100 - Loss 2.544324 - Tokens per Sec 14587.683005\n",
      " Step 1200 - Loss 2.589795 - Tokens per Sec 14610.532660\n",
      " Step 1300 - Loss 1.890549 - Tokens per Sec 14601.183247\n",
      " Step 1400 - Loss 1.774464 - Tokens per Sec 14590.093876\n",
      " Step 1500 - Loss 1.950664 - Tokens per Sec 14603.113108\n",
      " Step 1600 - Loss 2.476500 - Tokens per Sec 14628.233643\n",
      " Step 1700 - Loss 2.653585 - Tokens per Sec 14625.891074\n",
      " Step 1800 - Loss 3.009791 - Tokens per Sec 14633.861666\n",
      " Step 1900 - Loss 2.001644 - Tokens per Sec 14611.344536\n",
      " Step 2000 - Loss 2.320853 - Tokens per Sec 14602.664283\n",
      " Step 2100 - Loss 2.041386 - Tokens per Sec 14616.193478\n",
      " Step 2200 - Loss 3.027921 - Tokens per Sec 14612.955679\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: but the laws are a bit like , they 're all about each other .\n",
      "Expected: but the laws are kind of like sushi in a way : there are all kinds .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: this is an extraordinary for me . and it 's a soul , which is a solution that i 'm trying to me .\n",
      "Expected: and this is a legacy that was built for me , and it 's a legacy that i 've been cashing out on .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: second , we 're going to target .\n",
      "Expected: secondly , what about the aim ?\n",
      "\n",
      "\n",
      " Train loss 2.3586881291962225 | Validation loss 2.7293434034694326 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [8 / 100]\n",
      "\n",
      " Step 100 - Loss 2.114212 - Tokens per Sec 13913.269711\n",
      " Step 200 - Loss 2.574808 - Tokens per Sec 14217.898027\n",
      " Step 300 - Loss 2.089871 - Tokens per Sec 14277.488326\n",
      " Step 400 - Loss 2.755500 - Tokens per Sec 14386.783726\n",
      " Step 500 - Loss 2.362857 - Tokens per Sec 14468.352914\n",
      " Step 600 - Loss 1.841261 - Tokens per Sec 14474.559632\n",
      " Step 700 - Loss 2.142282 - Tokens per Sec 14481.862114\n",
      " Step 800 - Loss 2.220301 - Tokens per Sec 14512.447523\n",
      " Step 900 - Loss 2.037029 - Tokens per Sec 14556.491187\n",
      " Step 1000 - Loss 1.826513 - Tokens per Sec 14558.904954\n",
      " Step 1100 - Loss 2.234322 - Tokens per Sec 14562.144370\n",
      " Step 1200 - Loss 2.240017 - Tokens per Sec 14531.164122\n",
      " Step 1300 - Loss 2.428885 - Tokens per Sec 14552.157064\n",
      " Step 1400 - Loss 1.173066 - Tokens per Sec 14575.621602\n",
      " Step 1500 - Loss 2.313159 - Tokens per Sec 14567.602021\n",
      " Step 1600 - Loss 2.148599 - Tokens per Sec 14563.412813\n",
      " Step 1700 - Loss 2.653026 - Tokens per Sec 14567.400687\n",
      " Step 1800 - Loss 2.456407 - Tokens per Sec 14574.282000\n",
      " Step 1900 - Loss 1.680325 - Tokens per Sec 14590.705496\n",
      " Step 2000 - Loss 2.022694 - Tokens per Sec 14591.104076\n",
      " Step 2100 - Loss 2.870681 - Tokens per Sec 14610.770501\n",
      " Step 2200 - Loss 2.223245 - Tokens per Sec 14597.682816\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and it was like the mountain itself , i got to go through the day .\n",
      "Expected: and it really seemed like my concerns of the previous day vanishing .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: thank you .\n",
      "Expected: thank you .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: he does n't stop talking to him .\n",
      "Expected: it never stops coming .\n",
      "\n",
      "\n",
      " Train loss 2.252391909148792 | Validation loss 2.6943591182882134 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [9 / 100]\n",
      "\n",
      " Step 100 - Loss 2.404567 - Tokens per Sec 14245.339751\n",
      " Step 200 - Loss 1.699208 - Tokens per Sec 14461.803956\n",
      " Step 300 - Loss 2.426583 - Tokens per Sec 14571.328013\n",
      " Step 400 - Loss 1.810381 - Tokens per Sec 14303.923210\n",
      " Step 500 - Loss 1.445527 - Tokens per Sec 14415.462006\n",
      " Step 600 - Loss 2.768959 - Tokens per Sec 14448.833945\n",
      " Step 700 - Loss 1.507400 - Tokens per Sec 14530.993254\n",
      " Step 800 - Loss 2.663939 - Tokens per Sec 14562.492332\n",
      " Step 900 - Loss 1.952692 - Tokens per Sec 14509.662285\n",
      " Step 1000 - Loss 2.616740 - Tokens per Sec 14531.103083\n",
      " Step 1100 - Loss 2.005498 - Tokens per Sec 14544.239416\n",
      " Step 1200 - Loss 2.138780 - Tokens per Sec 14584.732109\n",
      " Step 1300 - Loss 1.668303 - Tokens per Sec 14574.603417\n",
      " Step 1400 - Loss 2.499885 - Tokens per Sec 14551.048293\n",
      " Step 1500 - Loss 2.548227 - Tokens per Sec 14583.170016\n",
      " Step 1600 - Loss 2.263751 - Tokens per Sec 14586.508378\n",
      " Step 1700 - Loss 2.684589 - Tokens per Sec 14598.419503\n",
      " Step 1800 - Loss 2.474083 - Tokens per Sec 14593.518438\n",
      " Step 1900 - Loss 2.401860 - Tokens per Sec 14602.669891\n",
      " Step 2000 - Loss 0.836249 - Tokens per Sec 14605.821267\n",
      " Step 2100 - Loss 1.483022 - Tokens per Sec 14603.184160\n",
      " Step 2200 - Loss 2.626871 - Tokens per Sec 14625.110014\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: women were dying for the british , the number of dictator , no one of their own .\n",
      "Expected: women came out , protested a brutal dictator , fearlessly spoke .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: and if you do n't believe , you ask your attention to the information that you have over you .\n",
      "Expected: and if you do n't believe me , ask your phone company what information they store about you .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: we live in the world today , and then our children in a kind of <unk> <unk> .\n",
      "Expected: now we live today , and are raising our children , in a kind of children's - fantasy - spectacular - industrial complex .\n",
      "\n",
      "\n",
      " Train loss 2.1604133194639688 | Validation loss 2.686795245517384 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [10 / 100]\n",
      "\n",
      " Step 100 - Loss 2.225067 - Tokens per Sec 13803.860154\n",
      " Step 200 - Loss 1.420803 - Tokens per Sec 13994.714473\n",
      " Step 300 - Loss 2.289850 - Tokens per Sec 14250.953204\n",
      " Step 400 - Loss 2.310624 - Tokens per Sec 14328.772531\n",
      " Step 500 - Loss 2.387013 - Tokens per Sec 14468.817648\n",
      " Step 600 - Loss 1.107530 - Tokens per Sec 14507.815138\n",
      " Step 700 - Loss 2.534847 - Tokens per Sec 14467.803456\n",
      " Step 800 - Loss 2.012566 - Tokens per Sec 14524.753679\n",
      " Step 900 - Loss 1.876251 - Tokens per Sec 14518.018956\n",
      " Step 1000 - Loss 1.860290 - Tokens per Sec 14551.413845\n",
      " Step 1100 - Loss 2.659972 - Tokens per Sec 14566.105712\n",
      " Step 1200 - Loss 2.709247 - Tokens per Sec 14562.920228\n",
      " Step 1300 - Loss 2.499466 - Tokens per Sec 14578.243881\n",
      " Step 1400 - Loss 1.893651 - Tokens per Sec 14567.836275\n",
      " Step 1500 - Loss 1.282538 - Tokens per Sec 14594.864800\n",
      " Step 1600 - Loss 2.069181 - Tokens per Sec 14590.732843\n",
      " Step 1700 - Loss 1.944089 - Tokens per Sec 14588.978194\n",
      " Step 1800 - Loss 2.069944 - Tokens per Sec 14596.074526\n",
      " Step 1900 - Loss 2.495725 - Tokens per Sec 14594.107237\n",
      " Step 2000 - Loss 1.977267 - Tokens per Sec 14597.951436\n",
      " Step 2100 - Loss 2.652833 - Tokens per Sec 14598.937670\n",
      " Step 2200 - Loss 2.605051 - Tokens per Sec 14597.266640\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: the neurogenesis is going to be <unk> .\n",
      "Expected: indeed , it will decrease neurogenesis .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: and my father -- here to see -- was the first one in his family , who was ever going to be a victim .\n",
      "Expected: and my father -- that 's him -- he was the first ever in his family to receive an education .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: five is <unk> .\n",
      "Expected: jealousy is exhausting .\n",
      "\n",
      "\n",
      " Train loss 2.079099310246439 | Validation loss 2.6815757101232354 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [11 / 100]\n",
      "\n",
      " Step 100 - Loss 2.093173 - Tokens per Sec 14299.612906\n",
      " Step 200 - Loss 2.403550 - Tokens per Sec 14542.563416\n",
      " Step 300 - Loss 1.786093 - Tokens per Sec 14712.567968\n",
      " Step 400 - Loss 1.693539 - Tokens per Sec 14580.477068\n",
      " Step 500 - Loss 2.518759 - Tokens per Sec 14642.015782\n",
      " Step 600 - Loss 2.398312 - Tokens per Sec 14721.858128\n",
      " Step 700 - Loss 1.740229 - Tokens per Sec 14714.573282\n",
      " Step 800 - Loss 0.999435 - Tokens per Sec 14734.893997\n",
      " Step 900 - Loss 1.612476 - Tokens per Sec 14638.871119\n",
      " Step 1000 - Loss 2.588188 - Tokens per Sec 14668.988709\n",
      " Step 1100 - Loss 1.602653 - Tokens per Sec 14655.480159\n",
      " Step 1200 - Loss 1.978133 - Tokens per Sec 14653.522858\n",
      " Step 1300 - Loss 2.411206 - Tokens per Sec 14686.463170\n",
      " Step 1400 - Loss 1.976539 - Tokens per Sec 14596.452607\n",
      " Step 1500 - Loss 1.087356 - Tokens per Sec 14610.432624\n",
      " Step 1600 - Loss 2.666623 - Tokens per Sec 14616.611721\n",
      " Step 1700 - Loss 1.993583 - Tokens per Sec 14624.248642\n",
      " Step 1800 - Loss 2.001026 - Tokens per Sec 14624.969918\n",
      " Step 1900 - Loss 1.793236 - Tokens per Sec 14585.407513\n",
      " Step 2000 - Loss 1.784586 - Tokens per Sec 14602.240321\n",
      " Step 2100 - Loss 2.785936 - Tokens per Sec 14604.343647\n",
      " Step 2200 - Loss 1.634402 - Tokens per Sec 14611.117186\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: i can take it from every angle and it 's always going to sound sound .\n",
      "Expected: i can photograph it from any angle , and it will still look 2d.\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i developed a computer for a <unk> <unk> , and every single project that we were on the legs , and i 'm going to be <unk> .\n",
      "Expected: i worked for an italian ngo , and every single project that we set up in africa failed .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: i really wanted to find a solution to make the local level .\n",
      "Expected: and i really wanted to find a solution that would respond to the local climate conditions .\n",
      "\n",
      "\n",
      " Train loss 2.0073089238837407 | Validation loss 2.658002333207564 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [12 / 100]\n",
      "\n",
      " Step 100 - Loss 2.213057 - Tokens per Sec 14084.922916\n",
      " Step 200 - Loss 2.209735 - Tokens per Sec 14056.497295\n",
      " Step 300 - Loss 1.848581 - Tokens per Sec 14258.281053\n",
      " Step 400 - Loss 2.026773 - Tokens per Sec 14368.197749\n",
      " Step 500 - Loss 2.333716 - Tokens per Sec 14408.928736\n",
      " Step 600 - Loss 2.486168 - Tokens per Sec 14462.917044\n",
      " Step 700 - Loss 1.629255 - Tokens per Sec 14332.382914\n",
      " Step 800 - Loss 1.708843 - Tokens per Sec 14408.233416\n",
      " Step 900 - Loss 1.756576 - Tokens per Sec 14427.398235\n",
      " Step 1000 - Loss 1.704201 - Tokens per Sec 14447.678759\n",
      " Step 1100 - Loss 2.377487 - Tokens per Sec 14441.588465\n",
      " Step 1200 - Loss 2.089165 - Tokens per Sec 14439.841841\n",
      " Step 1300 - Loss 2.623144 - Tokens per Sec 14485.418394\n",
      " Step 1400 - Loss 1.507123 - Tokens per Sec 14482.328036\n",
      " Step 1500 - Loss 2.081439 - Tokens per Sec 14490.669047\n",
      " Step 1600 - Loss 2.318106 - Tokens per Sec 14495.545564\n",
      " Step 1700 - Loss 2.085750 - Tokens per Sec 14493.396457\n",
      " Step 1800 - Loss 1.868698 - Tokens per Sec 14522.946251\n",
      " Step 1900 - Loss 1.162947 - Tokens per Sec 14532.969192\n",
      " Step 2000 - Loss 2.272595 - Tokens per Sec 14557.273750\n",
      " Step 2100 - Loss 1.806250 - Tokens per Sec 14538.531897\n",
      " Step 2200 - Loss 0.600471 - Tokens per Sec 14535.085941\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: they 're done by volunteer , the online are .\n",
      "Expected: these are managed by volunteer system administrators who are online .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: it 's a piece of country , we call <unk> .\n",
      "Expected: it was on a strip of land that we call a parkway .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: thank you very much .\n",
      "Expected: thank you .\n",
      "\n",
      "\n",
      " Train loss 1.9435119103602017 | Validation loss 2.6695908308029175 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [13 / 100]\n",
      "\n",
      " Step 100 - Loss 2.122016 - Tokens per Sec 14582.006332\n",
      " Step 200 - Loss 1.593517 - Tokens per Sec 14542.183722\n",
      " Step 300 - Loss 2.060268 - Tokens per Sec 14549.296090\n",
      " Step 400 - Loss 1.362449 - Tokens per Sec 14470.328463\n",
      " Step 500 - Loss 1.478005 - Tokens per Sec 14474.049452\n",
      " Step 600 - Loss 1.580716 - Tokens per Sec 14558.775228\n",
      " Step 700 - Loss 1.957819 - Tokens per Sec 14577.938930\n",
      " Step 800 - Loss 1.654829 - Tokens per Sec 14587.689489\n",
      " Step 900 - Loss 2.227501 - Tokens per Sec 14568.566703\n",
      " Step 1000 - Loss 1.580787 - Tokens per Sec 14569.433830\n",
      " Step 1100 - Loss 2.004407 - Tokens per Sec 14602.079492\n",
      " Step 1200 - Loss 1.431788 - Tokens per Sec 14611.977208\n",
      " Step 1300 - Loss 1.350776 - Tokens per Sec 14643.565645\n",
      " Step 1400 - Loss 1.741501 - Tokens per Sec 14596.097467\n",
      " Step 1500 - Loss 2.197056 - Tokens per Sec 14588.781210\n",
      " Step 1600 - Loss 1.797801 - Tokens per Sec 14600.327818\n",
      " Step 1700 - Loss 2.547797 - Tokens per Sec 14612.614323\n",
      " Step 1800 - Loss 0.489724 - Tokens per Sec 14625.111417\n",
      " Step 1900 - Loss 1.999512 - Tokens per Sec 14599.117779\n",
      " Step 2000 - Loss 1.309787 - Tokens per Sec 14590.545623\n",
      " Step 2100 - Loss 2.192384 - Tokens per Sec 14608.349050\n",
      " Step 2200 - Loss 2.180695 - Tokens per Sec 14613.263009\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: maybe they 'll be it .\n",
      "Expected: see if you can guess .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: he ca n't talk about it , but he 's thinking about a way that it could n't be a few of the best way .\n",
      "Expected: he 's speechless , but he communicates joy in a way that some of the best orators can not .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: and that was the beginning of a very short journey to me .\n",
      "Expected: and that was the beginning of a very strange journey for me .\n",
      "\n",
      "\n",
      " Train loss 1.8850239006919844 | Validation loss 2.6573204560713335 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [14 / 100]\n",
      "\n",
      " Step 100 - Loss 1.703431 - Tokens per Sec 13830.111950\n",
      " Step 200 - Loss 1.599470 - Tokens per Sec 14317.834262\n",
      " Step 300 - Loss 1.978440 - Tokens per Sec 14446.738479\n",
      " Step 400 - Loss 2.309129 - Tokens per Sec 14598.013663\n",
      " Step 500 - Loss 2.119429 - Tokens per Sec 14589.526873\n",
      " Step 600 - Loss 1.952327 - Tokens per Sec 14503.759143\n",
      " Step 700 - Loss 2.032164 - Tokens per Sec 14523.991114\n",
      " Step 800 - Loss 1.552610 - Tokens per Sec 14512.553074\n",
      " Step 900 - Loss 2.122533 - Tokens per Sec 14554.274809\n",
      " Step 1000 - Loss 2.084404 - Tokens per Sec 14550.369274\n",
      " Step 1100 - Loss 1.583913 - Tokens per Sec 14554.257210\n",
      " Step 1200 - Loss 2.046965 - Tokens per Sec 14514.952388\n",
      " Step 1300 - Loss 2.267372 - Tokens per Sec 14518.960151\n",
      " Step 1400 - Loss 1.738890 - Tokens per Sec 14549.285893\n",
      " Step 1500 - Loss 1.479667 - Tokens per Sec 14563.547364\n",
      " Step 1600 - Loss 1.717470 - Tokens per Sec 14573.697835\n",
      " Step 1700 - Loss 1.768766 - Tokens per Sec 14554.371295\n",
      " Step 1800 - Loss 2.048641 - Tokens per Sec 14562.450343\n",
      " Step 1900 - Loss 2.311609 - Tokens per Sec 14567.829753\n",
      " Step 2000 - Loss 2.078438 - Tokens per Sec 14571.700515\n",
      " Step 2100 - Loss 2.134369 - Tokens per Sec 14576.233899\n",
      " Step 2200 - Loss 2.227809 - Tokens per Sec 14580.827773\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: it 's not just the carbon dioxide that can be created such a pattern pattern of knowledge .\n",
      "Expected: it 's not only carbon dioxide that has this hockey stick pattern of accelerated change .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: please , do n't tell me that i 'm normal .\n",
      "Expected: please -- do n't tell me i 'm normal .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: trust you ? trust ? trust them ?\n",
      "Expected: do you trust politicians ? do you trust teachers ?\n",
      "\n",
      "\n",
      " Train loss 1.8319702058777407 | Validation loss 2.6716300682588057 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [15 / 100]\n",
      "\n",
      " Step 100 - Loss 1.568617 - Tokens per Sec 14041.212357\n",
      " Step 200 - Loss 1.526242 - Tokens per Sec 14568.120912\n",
      " Step 300 - Loss 1.776338 - Tokens per Sec 14576.226458\n",
      " Step 400 - Loss 1.569443 - Tokens per Sec 14553.703926\n",
      " Step 500 - Loss 1.056355 - Tokens per Sec 14555.779039\n",
      " Step 600 - Loss 0.534350 - Tokens per Sec 14533.373864\n",
      " Step 700 - Loss 2.112669 - Tokens per Sec 14603.521749\n",
      " Step 800 - Loss 1.164745 - Tokens per Sec 14603.780361\n",
      " Step 900 - Loss 1.475540 - Tokens per Sec 14562.040466\n",
      " Step 1000 - Loss 1.392450 - Tokens per Sec 14546.104346\n",
      " Step 1100 - Loss 1.959738 - Tokens per Sec 14546.943186\n",
      " Step 1200 - Loss 1.002293 - Tokens per Sec 14579.763018\n",
      " Step 1300 - Loss 1.935684 - Tokens per Sec 14572.801087\n",
      " Step 1400 - Loss 1.804908 - Tokens per Sec 14563.934097\n",
      " Step 1500 - Loss 2.170994 - Tokens per Sec 14572.095551\n",
      " Step 1600 - Loss 2.282309 - Tokens per Sec 14597.645479\n",
      " Step 1700 - Loss 2.285861 - Tokens per Sec 14604.169274\n",
      " Step 1800 - Loss 2.032251 - Tokens per Sec 14604.616011\n",
      " Step 1900 - Loss 2.158177 - Tokens per Sec 14622.816519\n",
      " Step 2000 - Loss 1.569625 - Tokens per Sec 14590.670170\n",
      " Step 2100 - Loss 2.257789 - Tokens per Sec 14604.308992\n",
      " Step 2200 - Loss 1.725689 - Tokens per Sec 14606.881065\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and they 're made up with you , you know , to stop doing something else .\n",
      "Expected: all of a sudden you 're telling them that they have to stop doing that to do something else .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: although i could n't really miss the eye life on the life , i had a plan .\n",
      "Expected: even though adjusting to life in south korea was not easy , i made a plan .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: where the scheduled of the scheduled of the scheduled is used to the war .\n",
      "Expected: we started to be able to see where the nile used to flow .\n",
      "\n",
      "\n",
      " Train loss 1.784140227253436 | Validation loss 2.6604192040183325 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [16 / 100]\n",
      "\n",
      " Step 100 - Loss 1.589959 - Tokens per Sec 14091.321296\n",
      " Step 200 - Loss 2.055972 - Tokens per Sec 14244.322502\n",
      " Step 300 - Loss 2.111429 - Tokens per Sec 14368.196208\n",
      " Step 400 - Loss 1.480649 - Tokens per Sec 14402.460706\n",
      " Step 500 - Loss 1.573396 - Tokens per Sec 14499.315246\n",
      " Step 600 - Loss 2.111012 - Tokens per Sec 14550.190298\n",
      " Step 700 - Loss 1.602420 - Tokens per Sec 14527.396830\n",
      " Step 800 - Loss 1.935165 - Tokens per Sec 14536.289428\n",
      " Step 900 - Loss 1.665978 - Tokens per Sec 14576.517612\n",
      " Step 1000 - Loss 2.215948 - Tokens per Sec 14600.507404\n",
      " Step 1100 - Loss 1.504092 - Tokens per Sec 14605.271847\n",
      " Step 1200 - Loss 1.630982 - Tokens per Sec 14589.342870\n",
      " Step 1300 - Loss 1.696576 - Tokens per Sec 14590.219258\n",
      " Step 1400 - Loss 1.672025 - Tokens per Sec 14617.140360\n",
      " Step 1500 - Loss 2.081527 - Tokens per Sec 14622.733176\n",
      " Step 1600 - Loss 2.130799 - Tokens per Sec 14617.491235\n",
      " Step 1700 - Loss 1.690111 - Tokens per Sec 14618.556939\n",
      " Step 1800 - Loss 2.028198 - Tokens per Sec 14604.566751\n",
      " Step 1900 - Loss 2.004911 - Tokens per Sec 14611.146859\n",
      " Step 2000 - Loss 1.234319 - Tokens per Sec 14618.821175\n",
      " Step 2100 - Loss 1.436021 - Tokens per Sec 14634.763613\n",
      " Step 2200 - Loss 1.353320 - Tokens per Sec 14628.796904\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: it 's continued to everything we 've now now .\n",
      "Expected: it 's far beyond what we currently have .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: yeah , i know , british rock , i 've been known as long as i have my own .\n",
      "Expected: but i know electric fences are already invented , but i want to make mine .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: his name is <unk> , and we were often amazed together ; we were very good friends .\n",
      "Expected: his name is paulie , and we worked together many times , and we became good friends .\n",
      "\n",
      "\n",
      " Train loss 1.7390822185620591 | Validation loss 2.6652560125697744 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [17 / 100]\n",
      "\n",
      " Step 100 - Loss 1.647976 - Tokens per Sec 14022.238769\n",
      " Step 200 - Loss 0.689720 - Tokens per Sec 14391.542971\n",
      " Step 300 - Loss 1.862077 - Tokens per Sec 14465.062563\n",
      " Step 400 - Loss 2.072742 - Tokens per Sec 14458.145205\n",
      " Step 500 - Loss 1.504554 - Tokens per Sec 14398.625306\n",
      " Step 600 - Loss 2.283685 - Tokens per Sec 14436.021057\n",
      " Step 700 - Loss 1.981359 - Tokens per Sec 14487.750204\n",
      " Step 800 - Loss 2.100091 - Tokens per Sec 14523.681208\n",
      " Step 900 - Loss 1.302039 - Tokens per Sec 14526.031406\n",
      " Step 1000 - Loss 2.076019 - Tokens per Sec 14542.743116\n",
      " Step 1100 - Loss 2.273966 - Tokens per Sec 14538.676340\n",
      " Step 1200 - Loss 1.505521 - Tokens per Sec 14547.364182\n",
      " Step 1300 - Loss 2.116747 - Tokens per Sec 14563.304362\n",
      " Step 1400 - Loss 1.887632 - Tokens per Sec 14587.053688\n",
      " Step 1500 - Loss 1.796637 - Tokens per Sec 14594.244977\n",
      " Step 1600 - Loss 1.646840 - Tokens per Sec 14566.261256\n",
      " Step 1700 - Loss 1.188271 - Tokens per Sec 14582.284227\n",
      " Step 1800 - Loss 2.219887 - Tokens per Sec 14575.572570\n",
      " Step 1900 - Loss 1.759280 - Tokens per Sec 14597.542422\n",
      " Step 2000 - Loss 1.579262 - Tokens per Sec 14587.070531\n",
      " Step 2100 - Loss 1.625036 - Tokens per Sec 14567.125428\n",
      " Step 2200 - Loss 1.653644 - Tokens per Sec 14580.892106\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: like saddam , like mugabe , like kim jong , <unk> , the <unk> or millions of millions .\n",
      "Expected: like saddam , like mugabe , like kim jong - il -- people who kill in hundreds of thousands or millions .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i was afraid .\n",
      "Expected: and i was scared .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: so i stayed one day , in my school , a longer longer , and i found myself in the <unk> .\n",
      "Expected: so one day i stayed a little late after school , a little too late , and i lurked in the girls ' bathroom .\n",
      "\n",
      "\n",
      " Train loss 1.697970736130679 | Validation loss 2.674288435415788 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [18 / 100]\n",
      "\n",
      " Step 100 - Loss 1.270468 - Tokens per Sec 14046.566542\n",
      " Step 200 - Loss 2.054479 - Tokens per Sec 14156.195303\n",
      " Step 300 - Loss 1.219064 - Tokens per Sec 14251.153501\n",
      " Step 400 - Loss 1.376264 - Tokens per Sec 14341.465967\n",
      " Step 500 - Loss 2.125976 - Tokens per Sec 14456.499910\n",
      " Step 600 - Loss 1.355632 - Tokens per Sec 14491.774938\n",
      " Step 700 - Loss 1.870298 - Tokens per Sec 14568.165846\n",
      " Step 800 - Loss 1.883526 - Tokens per Sec 14568.422150\n",
      " Step 900 - Loss 1.838898 - Tokens per Sec 14522.293767\n",
      " Step 1000 - Loss 1.453425 - Tokens per Sec 14556.017517\n",
      " Step 1100 - Loss 0.250057 - Tokens per Sec 14561.090980\n",
      " Step 1200 - Loss 1.583902 - Tokens per Sec 14605.386823\n",
      " Step 1300 - Loss 1.880526 - Tokens per Sec 14594.113061\n",
      " Step 1400 - Loss 2.007169 - Tokens per Sec 14563.347410\n",
      " Step 1500 - Loss 1.666030 - Tokens per Sec 14585.373775\n",
      " Step 1600 - Loss 1.183964 - Tokens per Sec 14592.959154\n",
      " Step 1700 - Loss 1.743547 - Tokens per Sec 14612.603351\n",
      " Step 1800 - Loss 1.939283 - Tokens per Sec 14603.655979\n",
      " Step 1900 - Loss 1.223360 - Tokens per Sec 14589.421949\n",
      " Step 2000 - Loss 1.340146 - Tokens per Sec 14577.811369\n",
      " Step 2100 - Loss 2.250632 - Tokens per Sec 14586.559641\n",
      " Step 2200 - Loss 1.551936 - Tokens per Sec 14603.456234\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and you have the ability to overcome the circumstances that they were born .\n",
      "Expected: and they can have the possibility of transcending the circumstances under which they were born .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: what you 're doing -- you 're the mouth .\n",
      "Expected: so what you do -- you shut up .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: but this explanation is not good enough .\n",
      "Expected: but i do n't think that explanation is good enough .\n",
      "\n",
      "\n",
      " Train loss 1.660349742077694 | Validation loss 2.6722530776804145 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [19 / 100]\n",
      "\n",
      " Step 100 - Loss 1.251499 - Tokens per Sec 14106.970621\n",
      " Step 200 - Loss 0.880117 - Tokens per Sec 14118.636846\n",
      " Step 300 - Loss 2.025920 - Tokens per Sec 14377.185759\n",
      " Step 400 - Loss 1.489395 - Tokens per Sec 14463.396890\n",
      " Step 500 - Loss 1.497830 - Tokens per Sec 14547.952103\n",
      " Step 600 - Loss 1.953261 - Tokens per Sec 14561.094973\n",
      " Step 700 - Loss 2.223438 - Tokens per Sec 14489.768294\n",
      " Step 800 - Loss 1.339114 - Tokens per Sec 14525.644952\n",
      " Step 900 - Loss 0.949419 - Tokens per Sec 14503.528822\n",
      " Step 1000 - Loss 2.196518 - Tokens per Sec 14548.483624\n",
      " Step 1100 - Loss 0.846502 - Tokens per Sec 14555.204250\n",
      " Step 1200 - Loss 1.914415 - Tokens per Sec 14540.579901\n",
      " Step 1300 - Loss 2.034363 - Tokens per Sec 14543.014725\n",
      " Step 1400 - Loss 1.805628 - Tokens per Sec 14547.012312\n",
      " Step 1500 - Loss 1.588907 - Tokens per Sec 14581.472508\n",
      " Step 1600 - Loss 1.392133 - Tokens per Sec 14575.879420\n",
      " Step 1700 - Loss 1.347216 - Tokens per Sec 14569.885058\n",
      " Step 1800 - Loss 2.095289 - Tokens per Sec 14571.131734\n",
      " Step 1900 - Loss 1.196443 - Tokens per Sec 14569.929253\n",
      " Step 2000 - Loss 1.550745 - Tokens per Sec 14590.366189\n",
      " Step 2100 - Loss 2.312571 - Tokens per Sec 14597.476126\n",
      " Step 2200 - Loss 2.326988 - Tokens per Sec 14595.404300\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: \" i 've enjoyed what most important vulnerability needs to do that like this ?\n",
      "Expected: \" did i evaluate what would be the highest pleasure like mill would ?\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: the last question of mine is me , \" how is it a matter of ? \"\n",
      "Expected: so the last question people ask me is , \" what is it like to be a model ? \"\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: it meant mainly what the silicon does n't really think of the valley .\n",
      "Expected: it mostly explains what 's wrong with silicon valley .\n",
      "\n",
      "\n",
      " Train loss 1.6262357454109149 | Validation loss 2.671834414655512 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [20 / 100]\n",
      "\n",
      " Step 100 - Loss 2.211869 - Tokens per Sec 14453.204125\n",
      " Step 200 - Loss 1.103348 - Tokens per Sec 14400.585786\n",
      " Step 300 - Loss 1.732338 - Tokens per Sec 14626.350501\n",
      " Step 400 - Loss 1.080695 - Tokens per Sec 14596.446159\n",
      " Step 500 - Loss 1.571818 - Tokens per Sec 14528.191841\n",
      " Step 600 - Loss 1.377397 - Tokens per Sec 14576.806096\n",
      " Step 700 - Loss 2.177953 - Tokens per Sec 14581.383708\n",
      " Step 800 - Loss 2.093030 - Tokens per Sec 14634.396558\n",
      " Step 900 - Loss 2.000438 - Tokens per Sec 14641.100404\n",
      " Step 1000 - Loss 1.593151 - Tokens per Sec 14606.222069\n",
      " Step 1100 - Loss 1.423714 - Tokens per Sec 14594.658895\n",
      " Step 1200 - Loss 2.003200 - Tokens per Sec 14606.383106\n",
      " Step 1300 - Loss 1.717181 - Tokens per Sec 14612.416671\n",
      " Step 1400 - Loss 1.229345 - Tokens per Sec 14613.450937\n",
      " Step 1500 - Loss 1.506971 - Tokens per Sec 14596.206782\n",
      " Step 1600 - Loss 2.121887 - Tokens per Sec 14592.360051\n",
      " Step 1700 - Loss 1.155048 - Tokens per Sec 14596.667994\n",
      " Step 1800 - Loss 2.054457 - Tokens per Sec 14594.596749\n",
      " Step 1900 - Loss 2.078201 - Tokens per Sec 14602.370063\n",
      " Step 2000 - Loss 1.025427 - Tokens per Sec 14582.763551\n",
      " Step 2100 - Loss 1.159120 - Tokens per Sec 14578.097379\n",
      " Step 2200 - Loss 1.627921 - Tokens per Sec 14590.895726\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: interestingly , it has been published before , but they did n't have the crossovers to it .\n",
      "Expected: interestingly , it 's done a crossover before , but not the crossovers that matter .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: and if you 're doing what you do is , what you do is <unk> .\n",
      "Expected: and when you 're describing your science , beware of jargon .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: so our goal is not really hard to recognize very difficult .\n",
      "Expected: so in the end , i think what we are aiming for is not very difficult to discern .\n",
      "\n",
      "\n",
      " Train loss 1.5918735386100153 | Validation loss 2.707065842368386 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [21 / 100]\n",
      "\n",
      " Step 100 - Loss 1.994866 - Tokens per Sec 14416.537864\n",
      " Step 200 - Loss 1.274269 - Tokens per Sec 14500.592448\n",
      " Step 300 - Loss 1.414722 - Tokens per Sec 14536.500040\n",
      " Step 400 - Loss 1.549852 - Tokens per Sec 14458.710634\n",
      " Step 500 - Loss 1.320628 - Tokens per Sec 14494.962482\n",
      " Step 600 - Loss 2.117156 - Tokens per Sec 14587.292628\n",
      " Step 700 - Loss 2.061953 - Tokens per Sec 14585.500186\n",
      " Step 800 - Loss 0.958705 - Tokens per Sec 14537.226476\n",
      " Step 900 - Loss 0.868389 - Tokens per Sec 14561.030474\n",
      " Step 1000 - Loss 0.317623 - Tokens per Sec 14560.198759\n",
      " Step 1100 - Loss 2.152511 - Tokens per Sec 14611.309007\n",
      " Step 1200 - Loss 2.164914 - Tokens per Sec 14607.927671\n",
      " Step 1300 - Loss 1.713942 - Tokens per Sec 14585.517582\n",
      " Step 1400 - Loss 0.931748 - Tokens per Sec 14582.644093\n",
      " Step 1500 - Loss 2.125683 - Tokens per Sec 14624.197608\n",
      " Step 1600 - Loss 1.865932 - Tokens per Sec 14627.858953\n",
      " Step 1700 - Loss 1.437094 - Tokens per Sec 14624.219970\n",
      " Step 1800 - Loss 1.599999 - Tokens per Sec 14617.801830\n",
      " Step 1900 - Loss 1.705042 - Tokens per Sec 14605.903139\n",
      " Step 2000 - Loss 1.105369 - Tokens per Sec 14628.088529\n",
      " Step 2100 - Loss 1.046083 - Tokens per Sec 14620.990579\n",
      " Step 2200 - Loss 2.182964 - Tokens per Sec 14624.895272\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and that fate of a trained , too .\n",
      "Expected: there 's an upside to the story .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: and we find out of the question and find what the person wants to do .\n",
      "Expected: and what we do , we become friends , and we find out what that person wants to do .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: to invest in the moon , approximately nasa needs about 1.2 billion dollars in the same value or four percent of the space .\n",
      "Expected: to get to the moon , nasa spent around 180 billion dollars in today 's money , or four percent of the federal budget .\n",
      "\n",
      "\n",
      " Train loss 1.5608301448219846 | Validation loss 2.705494024536826 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [22 / 100]\n",
      "\n",
      " Step 100 - Loss 0.269392 - Tokens per Sec 13741.499180\n",
      " Step 200 - Loss 2.085224 - Tokens per Sec 14169.505723\n",
      " Step 300 - Loss 1.398657 - Tokens per Sec 14230.742673\n",
      " Step 400 - Loss 0.423247 - Tokens per Sec 14410.389267\n",
      " Step 500 - Loss 1.486415 - Tokens per Sec 14435.603496\n",
      " Step 600 - Loss 1.935591 - Tokens per Sec 14394.228883\n",
      " Step 700 - Loss 2.101129 - Tokens per Sec 14431.875263\n",
      " Step 800 - Loss 2.099722 - Tokens per Sec 14505.318481\n",
      " Step 900 - Loss 2.128511 - Tokens per Sec 14511.666086\n",
      " Step 1000 - Loss 1.264533 - Tokens per Sec 14517.518424\n",
      " Step 1100 - Loss 1.866643 - Tokens per Sec 14522.527495\n",
      " Step 1200 - Loss 1.398665 - Tokens per Sec 14504.511773\n",
      " Step 1300 - Loss 1.415188 - Tokens per Sec 14541.463678\n",
      " Step 1400 - Loss 1.353098 - Tokens per Sec 14550.365877\n",
      " Step 1500 - Loss 2.162213 - Tokens per Sec 14560.039937\n",
      " Step 1600 - Loss 1.317558 - Tokens per Sec 14550.367630\n",
      " Step 1700 - Loss 1.471191 - Tokens per Sec 14517.387577\n",
      " Step 1800 - Loss 1.380089 - Tokens per Sec 14549.632154\n",
      " Step 1900 - Loss 1.380108 - Tokens per Sec 14551.996715\n",
      " Step 2000 - Loss 2.007362 - Tokens per Sec 14581.656017\n",
      " Step 2100 - Loss 1.376177 - Tokens per Sec 14567.213695\n",
      " Step 2200 - Loss 1.367194 - Tokens per Sec 14545.981072\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: that 's probably the best place in the world to see it .\n",
      "Expected: it 's probably the best place in the world to see them .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: so — — — the government says , \" make it again . \"\n",
      "Expected: so — — so the government says , \" do it again . \"\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: he starts to live a new life .\n",
      "Expected: he starts this new life .\n",
      "\n",
      "\n",
      " Train loss 1.5307844732428002 | Validation loss 2.709471745924516 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [23 / 100]\n",
      "\n",
      " Step 100 - Loss 1.840333 - Tokens per Sec 14071.158481\n",
      " Step 200 - Loss 1.352675 - Tokens per Sec 14396.676722\n",
      " Step 300 - Loss 1.642832 - Tokens per Sec 14446.113863\n",
      " Step 400 - Loss 1.411727 - Tokens per Sec 14422.557246\n",
      " Step 500 - Loss 1.863745 - Tokens per Sec 14424.956945\n",
      " Step 600 - Loss 0.852154 - Tokens per Sec 14528.146362\n",
      " Step 700 - Loss 1.945993 - Tokens per Sec 14577.983294\n",
      " Step 800 - Loss 1.412525 - Tokens per Sec 14579.244612\n",
      " Step 900 - Loss 2.013650 - Tokens per Sec 14558.528446\n",
      " Step 1000 - Loss 1.360471 - Tokens per Sec 14555.196290\n",
      " Step 1100 - Loss 1.960140 - Tokens per Sec 14597.547118\n",
      " Step 1200 - Loss 1.080889 - Tokens per Sec 14592.632408\n",
      " Step 1300 - Loss 1.858348 - Tokens per Sec 14611.875435\n",
      " Step 1400 - Loss 2.096400 - Tokens per Sec 14595.725246\n",
      " Step 1500 - Loss 1.183871 - Tokens per Sec 14560.838688\n",
      " Step 1600 - Loss 1.255462 - Tokens per Sec 14570.853652\n",
      " Step 1700 - Loss 0.936780 - Tokens per Sec 14574.185389\n",
      " Step 1800 - Loss 0.936631 - Tokens per Sec 14589.766607\n",
      " Step 1900 - Loss 1.981354 - Tokens per Sec 14577.482759\n",
      " Step 2000 - Loss 1.303607 - Tokens per Sec 14524.032611\n",
      " Step 2100 - Loss 1.769583 - Tokens per Sec 14538.208872\n",
      " Step 2200 - Loss 1.187415 - Tokens per Sec 14545.875674\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: i was being depressed .\n",
      "Expected: i did get depressed .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: over the years , you know , many people have done in many places .\n",
      "Expected: so great progress and treatment has been made over the years .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: it 's going to be because they 're based on some simple ideas about the hierarchy of our own power .\n",
      "Expected: they are tricks that work because they 're based on some pretty basic principles about how our brains work .\n",
      "\n",
      "\n",
      " Train loss 1.5034865182747412 | Validation loss 2.7185496633703057 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [24 / 100]\n",
      "\n",
      " Step 100 - Loss 1.421964 - Tokens per Sec 14249.359788\n",
      " Step 200 - Loss 1.048272 - Tokens per Sec 14409.457572\n",
      " Step 300 - Loss 1.473486 - Tokens per Sec 14421.897383\n",
      " Step 400 - Loss 1.634108 - Tokens per Sec 14519.154721\n",
      " Step 500 - Loss 1.446486 - Tokens per Sec 14549.669910\n",
      " Step 600 - Loss 1.410179 - Tokens per Sec 14554.103535\n",
      " Step 700 - Loss 1.642180 - Tokens per Sec 14546.158394\n",
      " Step 800 - Loss 0.767894 - Tokens per Sec 14534.475947\n",
      " Step 900 - Loss 1.712750 - Tokens per Sec 14564.904515\n",
      " Step 1000 - Loss 1.311542 - Tokens per Sec 14555.817709\n",
      " Step 1100 - Loss 1.619544 - Tokens per Sec 14595.308178\n",
      " Step 1200 - Loss 1.946792 - Tokens per Sec 14581.210152\n",
      " Step 1300 - Loss 2.171282 - Tokens per Sec 14541.838974\n",
      " Step 1400 - Loss 1.718257 - Tokens per Sec 14556.678335\n",
      " Step 1500 - Loss 1.113878 - Tokens per Sec 14553.958576\n",
      " Step 1600 - Loss 1.479610 - Tokens per Sec 14581.506769\n",
      " Step 1700 - Loss 1.506820 - Tokens per Sec 14582.443714\n",
      " Step 1800 - Loss 0.953843 - Tokens per Sec 14535.461051\n",
      " Step 1900 - Loss 1.968774 - Tokens per Sec 14561.917758\n",
      " Step 2000 - Loss 1.910219 - Tokens per Sec 14566.077113\n",
      " Step 2100 - Loss 1.315291 - Tokens per Sec 14591.082084\n",
      " Step 2200 - Loss 1.292255 - Tokens per Sec 14597.815840\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: ss : is it safe to know ?\n",
      "Expected: ss : but is it safe to use everyday ?\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i was desperate .\n",
      "Expected: and i was distraught .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: hollywood is <unk> <unk> to buy other actors .\n",
      "Expected: hollywood has a sordid history of casting able - bodied actors to play disabled onscreen .\n",
      "\n",
      "\n",
      " Train loss 1.4770701087554972 | Validation loss 2.7230996001850474 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [25 / 100]\n",
      "\n",
      " Step 100 - Loss 1.797063 - Tokens per Sec 13994.582229\n",
      " Step 200 - Loss 0.542046 - Tokens per Sec 14447.102201\n",
      " Step 300 - Loss 1.108392 - Tokens per Sec 14504.764562\n",
      " Step 400 - Loss 1.438671 - Tokens per Sec 14567.356579\n",
      " Step 500 - Loss 0.848238 - Tokens per Sec 14527.594025\n",
      " Step 600 - Loss 1.218420 - Tokens per Sec 14496.317741\n",
      " Step 700 - Loss 1.947664 - Tokens per Sec 14557.425451\n",
      " Step 800 - Loss 1.194499 - Tokens per Sec 14538.302577\n",
      " Step 900 - Loss 1.156899 - Tokens per Sec 14584.852646\n",
      " Step 1000 - Loss 1.604797 - Tokens per Sec 14588.907877\n",
      " Step 1100 - Loss 1.305998 - Tokens per Sec 14530.719664\n",
      " Step 1200 - Loss 1.664216 - Tokens per Sec 14562.966715\n",
      " Step 1300 - Loss 1.484492 - Tokens per Sec 14559.536542\n",
      " Step 1400 - Loss 1.597912 - Tokens per Sec 14586.193629\n",
      " Step 1500 - Loss 2.081405 - Tokens per Sec 14589.610575\n",
      " Step 1600 - Loss 1.834435 - Tokens per Sec 14553.271359\n",
      " Step 1700 - Loss 1.551514 - Tokens per Sec 14568.316654\n",
      " Step 1800 - Loss 1.339000 - Tokens per Sec 14579.141655\n",
      " Step 1900 - Loss 1.722146 - Tokens per Sec 14596.841556\n",
      " Step 2000 - Loss 1.124514 - Tokens per Sec 14589.095021\n",
      " Step 2100 - Loss 1.501101 - Tokens per Sec 14577.619546\n",
      " Step 2200 - Loss 1.968118 - Tokens per Sec 14580.732371\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: some mammals became back to the water .\n",
      "Expected: some mammals turned back to water .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: thank you very much .\n",
      "Expected: thank you .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: at the state , they were all wondering : lust , fear and social security -- interesting .\n",
      "Expected: for the buddha , they are all psychological : lust , fear and social duty -- interesting .\n",
      "\n",
      "\n",
      " Train loss 1.4531439028790234 | Validation loss 2.7367640300230547 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [26 / 100]\n",
      "\n",
      " Step 100 - Loss 1.600253 - Tokens per Sec 14216.813925\n",
      " Step 200 - Loss 0.783655 - Tokens per Sec 14542.519840\n",
      " Step 300 - Loss 0.716328 - Tokens per Sec 14535.800068\n",
      " Step 400 - Loss 2.017679 - Tokens per Sec 14405.942442\n",
      " Step 500 - Loss 1.384347 - Tokens per Sec 14491.485036\n",
      " Step 600 - Loss 1.208043 - Tokens per Sec 14501.850921\n",
      " Step 700 - Loss 1.419279 - Tokens per Sec 14578.634283\n",
      " Step 800 - Loss 1.178951 - Tokens per Sec 14562.325237\n",
      " Step 900 - Loss 1.025041 - Tokens per Sec 14440.196573\n",
      " Step 1000 - Loss 1.019666 - Tokens per Sec 14495.239436\n",
      " Step 1100 - Loss 1.613640 - Tokens per Sec 14508.193211\n",
      " Step 1200 - Loss 1.349566 - Tokens per Sec 14534.341181\n",
      " Step 1300 - Loss 1.137727 - Tokens per Sec 14546.949894\n",
      " Step 1400 - Loss 1.531112 - Tokens per Sec 14518.353072\n",
      " Step 1500 - Loss 1.627914 - Tokens per Sec 14518.758907\n",
      " Step 1600 - Loss 1.237678 - Tokens per Sec 14519.641017\n",
      " Step 1700 - Loss 1.113018 - Tokens per Sec 14541.081034\n",
      " Step 1800 - Loss 1.839167 - Tokens per Sec 14537.847149\n",
      " Step 1900 - Loss 1.756576 - Tokens per Sec 14524.177120\n",
      " Step 2000 - Loss 1.675720 - Tokens per Sec 14539.130552\n",
      " Step 2100 - Loss 0.408613 - Tokens per Sec 14543.801955\n",
      " Step 2200 - Loss 1.843877 - Tokens per Sec 14567.280565\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: a lot of the effort of humanizing classrooms are focused on to the students .\n",
      "Expected: a lot of the effort in humanizing the classroom is focused on student - to - teacher ratios .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: this is the river <unk> , which is located in the warm of the north and north america .\n",
      "Expected: this is the amrok river , which serves as a part of the border between north korea and china .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: <unk> , diamonds , delicious -- that 's the way to the wisdom of wisdom .\n",
      "Expected: grief , humiliation , loss : these were the avenues to wisdom for proust .\n",
      "\n",
      "\n",
      " Train loss 1.4291130399592071 | Validation loss 2.7441080808639526 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [27 / 100]\n",
      "\n",
      " Step 100 - Loss 1.569657 - Tokens per Sec 13853.275604\n",
      " Step 200 - Loss 1.735886 - Tokens per Sec 13743.737303\n",
      " Step 300 - Loss 1.199785 - Tokens per Sec 14086.668983\n",
      " Step 400 - Loss 1.167079 - Tokens per Sec 14225.135507\n",
      " Step 500 - Loss 2.155342 - Tokens per Sec 14396.162584\n",
      " Step 600 - Loss 1.352646 - Tokens per Sec 14314.743910\n",
      " Step 700 - Loss 0.713611 - Tokens per Sec 14323.329644\n",
      " Step 800 - Loss 1.725025 - Tokens per Sec 14373.583636\n",
      " Step 900 - Loss 1.599115 - Tokens per Sec 14388.321076\n",
      " Step 1000 - Loss 0.812593 - Tokens per Sec 14442.185686\n",
      " Step 1100 - Loss 1.506511 - Tokens per Sec 14410.753033\n",
      " Step 1200 - Loss 0.999686 - Tokens per Sec 14427.584417\n",
      " Step 1300 - Loss 1.855047 - Tokens per Sec 14466.247230\n",
      " Step 1400 - Loss 1.240238 - Tokens per Sec 14474.300732\n",
      " Step 1500 - Loss 1.932502 - Tokens per Sec 14513.345247\n",
      " Step 1600 - Loss 1.461341 - Tokens per Sec 14481.043870\n",
      " Step 1700 - Loss 1.323229 - Tokens per Sec 14490.583218\n",
      " Step 1800 - Loss 1.996857 - Tokens per Sec 14493.687964\n",
      " Step 1900 - Loss 1.997097 - Tokens per Sec 14500.317061\n",
      " Step 2000 - Loss 1.591718 - Tokens per Sec 14520.768555\n",
      " Step 2100 - Loss 1.745968 - Tokens per Sec 14517.355554\n",
      " Step 2200 - Loss 0.431582 - Tokens per Sec 14491.094120\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: i mean , what is a biosphere ?\n",
      "Expected: i mean , what is a biosphere ?\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: you 're mad at other media inventions .\n",
      "Expected: you 're working on other electrical inventions .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: last year , the new york times a modern was published a government that had the government .\n",
      "Expected: last year , the new york times published a study that the government had done .\n",
      "\n",
      "\n",
      " Train loss 1.4067451740662116 | Validation loss 2.7570924000306563 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [28 / 100]\n",
      "\n",
      " Step 100 - Loss 0.910627 - Tokens per Sec 14428.423772\n",
      " Step 200 - Loss 1.715891 - Tokens per Sec 14618.891817\n",
      " Step 300 - Loss 1.143725 - Tokens per Sec 14658.470139\n",
      " Step 400 - Loss 0.847653 - Tokens per Sec 14472.603648\n",
      " Step 500 - Loss 1.365662 - Tokens per Sec 14467.335941\n",
      " Step 600 - Loss 1.547731 - Tokens per Sec 14555.883582\n",
      " Step 700 - Loss 1.110142 - Tokens per Sec 14537.611207\n",
      " Step 800 - Loss 1.001853 - Tokens per Sec 14579.200867\n",
      " Step 900 - Loss 1.404873 - Tokens per Sec 14520.299265\n",
      " Step 1000 - Loss 1.606729 - Tokens per Sec 14546.559468\n",
      " Step 1100 - Loss 1.544224 - Tokens per Sec 14531.846968\n",
      " Step 1200 - Loss 0.983728 - Tokens per Sec 14561.674726\n",
      " Step 1300 - Loss 1.871518 - Tokens per Sec 14587.733374\n",
      " Step 1400 - Loss 0.250342 - Tokens per Sec 14563.768383\n",
      " Step 1500 - Loss 1.135550 - Tokens per Sec 14573.078821\n",
      " Step 1600 - Loss 1.726917 - Tokens per Sec 14577.233876\n",
      " Step 1700 - Loss 1.644977 - Tokens per Sec 14570.118910\n",
      " Step 1800 - Loss 1.242615 - Tokens per Sec 14596.106319\n",
      " Step 1900 - Loss 1.615176 - Tokens per Sec 14568.311788\n",
      " Step 2000 - Loss 1.171020 - Tokens per Sec 14578.559336\n",
      " Step 2100 - Loss 1.049373 - Tokens per Sec 14585.495233\n",
      " Step 2200 - Loss 1.804108 - Tokens per Sec 14605.727586\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: last year , we had 450,000 billion dollars in <unk> alone in the united states .\n",
      "Expected: last year saw 997 billion dollars in corporate fraud alone in the united states .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: so , from reading a one of what we have made .\n",
      "Expected: read it from an african woman , the damage that we have done .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: the key on the left is two panels to be <unk> .\n",
      "Expected: the character on the left is two mountains stacked on top of each other .\n",
      "\n",
      "\n",
      " Train loss 1.385073291007088 | Validation loss 2.7561602158979936 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [29 / 100]\n",
      "\n",
      " Step 100 - Loss 1.728016 - Tokens per Sec 14389.336979\n",
      " Step 200 - Loss 1.726552 - Tokens per Sec 14156.489660\n",
      " Step 300 - Loss 1.958176 - Tokens per Sec 14256.919696\n",
      " Step 400 - Loss 1.269385 - Tokens per Sec 14338.222034\n",
      " Step 500 - Loss 1.272505 - Tokens per Sec 14372.980231\n",
      " Step 600 - Loss 1.687587 - Tokens per Sec 14464.811395\n",
      " Step 700 - Loss 1.126217 - Tokens per Sec 14358.444635\n",
      " Step 800 - Loss 2.008822 - Tokens per Sec 14425.431161\n",
      " Step 900 - Loss 1.648789 - Tokens per Sec 14449.334830\n",
      " Step 1000 - Loss 1.763225 - Tokens per Sec 14475.851862\n",
      " Step 1100 - Loss 0.836130 - Tokens per Sec 14508.458390\n",
      " Step 1200 - Loss 0.534406 - Tokens per Sec 14461.840550\n",
      " Step 1300 - Loss 1.861696 - Tokens per Sec 14484.665891\n",
      " Step 1400 - Loss 1.054073 - Tokens per Sec 14501.512265\n",
      " Step 1500 - Loss 1.424031 - Tokens per Sec 14518.486392\n",
      " Step 1600 - Loss 0.990416 - Tokens per Sec 14532.245608\n",
      " Step 1700 - Loss 1.751792 - Tokens per Sec 14506.694300\n",
      " Step 1800 - Loss 0.946886 - Tokens per Sec 14509.994069\n",
      " Step 1900 - Loss 1.785728 - Tokens per Sec 14506.313393\n",
      " Step 2000 - Loss 1.687925 - Tokens per Sec 14524.590343\n",
      " Step 2100 - Loss 1.437075 - Tokens per Sec 14527.123593\n",
      " Step 2200 - Loss 1.861062 - Tokens per Sec 14523.636986\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: or <unk> <unk> , <unk> , the <unk> of the designer , and even pirated of religious evangelical .\n",
      "Expected: you can buy cuecas baratas , designer underwear that is n't really manufactured by a designer , and even pirated evangelical mixtapes .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: absolutely little . the infrastructure is already .\n",
      "Expected: the neurologist 's test is non - invasive . they both use existing infrastructure .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: <unk> 's name earlier .\n",
      "Expected: jealousy likes photos .\n",
      "\n",
      "\n",
      " Train loss 1.3656670217555071 | Validation loss 2.7796613086353648 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = 65646\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f'Epoch [{epoch} / {num_epochs}]')\n",
    "\n",
    "\n",
    "    loss =  run_epoch((rebatch(b) for b in train_iterator))\n",
    "    validation_loss = run_validation((rebatch(b) for b in valid_iterator))\n",
    "    \n",
    "    \n",
    "    rand_i01 = np.random.randint(0, len(train_data))\n",
    "    rand_i02 = np.random.randint(0, len(valid_data))\n",
    "    rand_i03 = np.random.randint(0, len(test_data))\n",
    "    \n",
    "    sentence01, expected01 = \" \".join(train_data[rand_i01].src), \" \".join(train_data[rand_i01].trg)\n",
    "    sentence02, expected02 = \" \".join(valid_data[rand_i02].src), \" \".join(valid_data[rand_i02].trg)\n",
    "    sentence03, expected03 = \" \".join(test_data[rand_i03].src), \" \".join(test_data[rand_i03].trg)\n",
    "    \n",
    "    translated_sentence01 = translate_sentence_bahdanau(model, sentence01, max_length=50)\n",
    "    translated_sentence02 = translate_sentence_bahdanau(model, sentence02, max_length=50)\n",
    "    translated_sentence03 = translate_sentence_bahdanau(model, sentence03, max_length=50)\n",
    "    #out = beam(sentence, 3)  list(map(convert, out[:2]))\n",
    "    \n",
    "    \n",
    "    print(f\"\\nExample #1 (from Train data): \\nTranslation: { translated_sentence01 }\\nExpected: { expected01 }\")\n",
    "    print(f\"\\nExample #2 (from Validation): \\nTranslation: { translated_sentence02 }\\nExpected: { expected02 }\")\n",
    "    print(f\"\\nExample #3 (from Test data): \\nTranslation: { translated_sentence03 }\\nExpected: { expected03 }\\n\")\n",
    "    \n",
    "    print(f\"\\n Train loss {loss} | Validation loss {validation_loss} \\n\\n\\n\")\n",
    "    \n",
    "    if validation_loss < best_loss:\n",
    "        torch.save(model.state_dict(), \"best_model\")\n",
    "        best_loss = validation_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "papermill": {
   "duration": 4471.959812,
   "end_time": "2021-01-27T23:27:49.981163",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-27T22:13:18.021351",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
