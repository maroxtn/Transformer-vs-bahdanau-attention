{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 4.174455,
     "end_time": "2021-01-27T23:32:57.429683",
     "exception": false,
     "start_time": "2021-01-27T23:32:53.255228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import IWSLT\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:32:57.471593Z",
     "iopub.status.busy": "2021-01-27T23:32:57.470817Z",
     "iopub.status.idle": "2021-01-27T23:33:11.454277Z",
     "shell.execute_reply": "2021-01-27T23:33:11.452689Z"
    },
    "papermill": {
     "duration": 14.008622,
     "end_time": "2021-01-27T23:33:11.454432",
     "exception": false,
     "start_time": "2021-01-27T23:32:57.445810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/maroxtn/IWSLT-BACKUP/archive/main.zip -q\n",
    "!unzip -q main.zip\n",
    "\n",
    "!mkdir .data\n",
    "!mv IWSLT-BACKUP-main/iwslt .data\n",
    "\n",
    "!rm -rf main.zip IWSLT-BACKUP-main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:33:11.509310Z",
     "iopub.status.busy": "2021-01-27T23:33:11.508404Z",
     "iopub.status.idle": "2021-01-27T23:33:11.510707Z",
     "shell.execute_reply": "2021-01-27T23:33:11.510042Z"
    },
    "papermill": {
     "duration": 0.032975,
     "end_time": "2021-01-27T23:33:11.510826",
     "exception": false,
     "start_time": "2021-01-27T23:33:11.477851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CFG = {\"IN_LANG\":\"de\", \"OUT_LANG\": \"en\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-27T23:33:11.579018Z",
     "iopub.status.busy": "2021-01-27T23:33:11.578290Z",
     "iopub.status.idle": "2021-01-27T23:33:39.332667Z",
     "shell.execute_reply": "2021-01-27T23:33:39.332181Z"
    },
    "papermill": {
     "duration": 27.798547,
     "end_time": "2021-01-27T23:33:39.332773",
     "exception": false,
     "start_time": "2021-01-27T23:33:11.534226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "if CFG[\"IN_LANG\"] == \"en\":\n",
    "    spacy_in_lang = en_core_web_sm.load()\n",
    "    spacy_out_lang = de_core_news_sm.load()\n",
    "else:\n",
    "    spacy_in_lang = de_core_news_sm.load()\n",
    "    spacy_out_lang = en_core_web_sm.load()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:33:39.402781Z",
     "iopub.status.busy": "2021-01-27T23:33:39.402049Z",
     "iopub.status.idle": "2021-01-27T23:33:39.405084Z",
     "shell.execute_reply": "2021-01-27T23:33:39.404633Z"
    },
    "papermill": {
     "duration": 0.040147,
     "end_time": "2021-01-27T23:33:39.405182",
     "exception": false,
     "start_time": "2021-01-27T23:33:39.365035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenizer_in(text):\n",
    "    return [tok.text for tok in spacy_in_lang.tokenizer(text)]\n",
    "\n",
    "def tokenizer_out(text):\n",
    "    return [tok.text for tok in spacy_out_lang.tokenizer(text)]\n",
    "\n",
    "in_lang = Field(tokenize=tokenizer_in, lower=True)\n",
    "out_lang = Field(tokenize=tokenizer_out, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:33:39.472063Z",
     "iopub.status.busy": "2021-01-27T23:33:39.471529Z",
     "iopub.status.idle": "2021-01-27T23:34:55.899687Z",
     "shell.execute_reply": "2021-01-27T23:34:55.898824Z"
    },
    "papermill": {
     "duration": 76.464042,
     "end_time": "2021-01-27T23:34:55.899811",
     "exception": false,
     "start_time": "2021-01-27T23:33:39.435769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 25\n",
    "\n",
    "train_data, valid_data, test_data = IWSLT.splits(\n",
    "        exts=(\".\"+CFG[\"IN_LANG\"], \".\"+CFG[\"OUT_LANG\"]), fields=(in_lang, out_lang ),filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "            len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:55.994758Z",
     "iopub.status.busy": "2021-01-27T23:34:55.984511Z",
     "iopub.status.idle": "2021-01-27T23:34:57.514134Z",
     "shell.execute_reply": "2021-01-27T23:34:57.513632Z"
    },
    "papermill": {
     "duration": 1.583006,
     "end_time": "2021-01-27T23:34:57.514239",
     "exception": false,
     "start_time": "2021-01-27T23:34:55.931233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_lang.build_vocab(train_data, min_freq=2)\n",
    "out_lang.build_vocab(train_data, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:57.587892Z",
     "iopub.status.busy": "2021-01-27T23:34:57.587018Z",
     "iopub.status.idle": "2021-01-27T23:34:57.589688Z",
     "shell.execute_reply": "2021-01-27T23:34:57.589276Z"
    },
    "papermill": {
     "duration": 0.044374,
     "end_time": "2021-01-27T23:34:57.589777",
     "exception": false,
     "start_time": "2021-01-27T23:34:57.545403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(\n",
    "            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.scale * self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:57.671496Z",
     "iopub.status.busy": "2021-01-27T23:34:57.670755Z",
     "iopub.status.idle": "2021-01-27T23:34:57.673624Z",
     "shell.execute_reply": "2021-01-27T23:34:57.673212Z"
    },
    "papermill": {
     "duration": 0.053416,
     "end_time": "2021-01-27T23:34:57.673711",
     "exception": false,
     "start_time": "2021-01-27T23:34:57.620295",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, intoken, outtoken ,hidden, enc_layers=2, dec_layers=2, dropout=.1, nheads=2, ff_model=128):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(intoken, hidden)\n",
    "        self.pos_encoder = PositionalEncoding(hidden, dropout)\n",
    "\n",
    "        self.decoder = nn.Embedding(outtoken, hidden) \n",
    "        self.pos_decoder = PositionalEncoding(hidden, dropout)\n",
    "        \n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(d_model=hidden, nhead = nheads, dim_feedforward = ff_model, dropout=dropout, activation='relu')\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, enc_layers)\n",
    "\n",
    "        encoder_layers = TransformerDecoderLayer(hidden, nheads, ff_model, dropout, activation='relu')\n",
    "        self.transformer_decoder = TransformerDecoder(encoder_layers, dec_layers)        \n",
    "\n",
    "        self.fc_out = nn.Linear(hidden, outtoken)\n",
    "\n",
    "        self.src_mask = None\n",
    "        self.trg_mask = None\n",
    "        self.memory_mask = None\n",
    "\n",
    "        \n",
    "    def generate_square_subsequent_mask(self, sz, sz1=None):\n",
    "        \n",
    "        if sz1 == None:\n",
    "            mask = torch.triu(torch.ones(sz, sz), 1)\n",
    "        else:\n",
    "            mask = torch.triu(torch.ones(sz, sz1), 1)\n",
    "            \n",
    "        return mask.masked_fill(mask==1, float('-inf'))\n",
    "\n",
    "    def make_len_mask_enc(self, inp):\n",
    "        return (inp == in_pad_idx).transpose(0, 1)   #(batch_size, output_seq_len)\n",
    "    \n",
    "    def make_len_mask_dec(self, inp):\n",
    "        return (inp == out_pad_idx).transpose(0, 1) #(batch_size, input_seq_len)\n",
    "    \n",
    "\n",
    "\n",
    "    def forward(self, src, trg): #SRC: (seq_len, batch_size)\n",
    "\n",
    "        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n",
    "            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device)\n",
    "            \n",
    "\n",
    "        #Adding padding mask\n",
    "        src_pad_mask = self.make_len_mask_enc(src)\n",
    "        trg_pad_mask = self.make_len_mask_dec(trg)\n",
    "             \n",
    "\n",
    "        #Add embeddings Encoder\n",
    "        src = self.encoder(src)  #Embedding, (seq_len, batch_size, d_model)\n",
    "        src = self.pos_encoder(src)   #Pos embedding\n",
    "        \n",
    "        \n",
    "        #Add embedding decoder\n",
    "        trg = self.decoder(trg) #(seq_len, batch_size, d_model)\n",
    "        trg = self.pos_decoder(trg)\n",
    "\n",
    "        \n",
    "        memory = self.transformer_encoder(src, None, src_pad_mask)\n",
    "        output = self.transformer_decoder(tgt = trg, memory = memory, tgt_mask = self.trg_mask, memory_mask = None, \n",
    "                                          tgt_key_padding_mask = trg_pad_mask, memory_key_padding_mask = src_pad_mask)\n",
    "\n",
    "        output = self.fc_out(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:57.743302Z",
     "iopub.status.busy": "2021-01-27T23:34:57.742555Z",
     "iopub.status.idle": "2021-01-27T23:34:57.745562Z",
     "shell.execute_reply": "2021-01-27T23:34:57.745117Z"
    },
    "papermill": {
     "duration": 0.038648,
     "end_time": "2021-01-27T23:34:57.745650",
     "exception": false,
     "start_time": "2021-01-27T23:34:57.707002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rebatch(batch):\n",
    "    return Batch(batch.src, batch.trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:57.817913Z",
     "iopub.status.busy": "2021-01-27T23:34:57.817185Z",
     "iopub.status.idle": "2021-01-27T23:34:57.820083Z",
     "shell.execute_reply": "2021-01-27T23:34:57.819661Z"
    },
    "papermill": {
     "duration": 0.042817,
     "end_time": "2021-01-27T23:34:57.820180",
     "exception": false,
     "start_time": "2021-01-27T23:34:57.777363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Batch:\n",
    "\n",
    "    def __init__(self, src, trg):\n",
    "        \n",
    "        self.src = src\n",
    "        \n",
    "        self.trg = None\n",
    "        self.trg_y = None\n",
    "        self.ntokens = None\n",
    "\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:-1,]\n",
    "            self.trg_y = trg[1:].reshape(-1)\n",
    "            self.ntokens = (self.trg_y != out_pad_idx).sum().item()  \n",
    "        \n",
    "        if device == torch.device('cuda'):\n",
    "            self.src = self.src.cuda()\n",
    "\n",
    "            if trg is not None:\n",
    "                self.trg = self.trg.cuda()\n",
    "                self.trg_y = self.trg_y.cuda()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:57.897083Z",
     "iopub.status.busy": "2021-01-27T23:34:57.896216Z",
     "iopub.status.idle": "2021-01-27T23:34:57.899113Z",
     "shell.execute_reply": "2021-01-27T23:34:57.898653Z"
    },
    "papermill": {
     "duration": 0.045682,
     "end_time": "2021-01-27T23:34:57.899209",
     "exception": false,
     "start_time": "2021-01-27T23:34:57.853527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    \"Optim wrapper that implements rate.\"\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "        \n",
    "    def step(self):\n",
    "        \"Update parameters and rate\"\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def rate(self, step = None):\n",
    "        \"Implement `lrate` above\"\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * \\\n",
    "            (self.model_size ** (-0.5) *\n",
    "            min(step ** (-0.5), step * self.warmup ** (-1.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:58.004844Z",
     "iopub.status.busy": "2021-01-27T23:34:58.004075Z",
     "iopub.status.idle": "2021-01-27T23:34:58.006965Z",
     "shell.execute_reply": "2021-01-27T23:34:58.006538Z"
    },
    "papermill": {
     "duration": 0.038978,
     "end_time": "2021-01-27T23:34:58.007090",
     "exception": false,
     "start_time": "2021-01-27T23:34:57.968112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Training Hyperparameters\n",
    "num_epochs = 30\n",
    "batch_size = 256\n",
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:34:58.448697Z",
     "iopub.status.busy": "2021-01-27T23:34:58.445211Z",
     "iopub.status.idle": "2021-01-27T23:35:03.224335Z",
     "shell.execute_reply": "2021-01-27T23:35:03.223251Z"
    },
    "papermill": {
     "duration": 5.184667,
     "end_time": "2021-01-27T23:35:03.224450",
     "exception": false,
     "start_time": "2021-01-27T23:34:58.039783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model Hyperparameter\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size_encoder = len(in_lang.vocab)\n",
    "input_size_decoder = len(out_lang.vocab)\n",
    "output_size = len(out_lang.vocab)\n",
    "\n",
    "d_model = 256\n",
    "\n",
    "import math \n",
    "\n",
    "model = TransformerModel(input_size_encoder, input_size_decoder ,d_model, enc_layers=1, dec_layers=1, dropout=.1, nheads=1, ff_model=1028).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.297058Z",
     "iopub.status.busy": "2021-01-27T23:35:03.296300Z",
     "iopub.status.idle": "2021-01-27T23:35:03.298732Z",
     "shell.execute_reply": "2021-01-27T23:35:03.299173Z"
    },
    "papermill": {
     "duration": 0.042478,
     "end_time": "2021-01-27T23:35:03.299304",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.256826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_pad_idx = in_lang.vocab.stoi['<pad>']\n",
    "out_pad_idx = out_lang.vocab.stoi['<pad>']\n",
    "\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=out_pad_idx)\n",
    "optimizer = NoamOpt(d_model, 1, 4000 ,optim.Adam(model.parameters(), lr=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.375451Z",
     "iopub.status.busy": "2021-01-27T23:35:03.374751Z",
     "iopub.status.idle": "2021-01-27T23:35:03.377633Z",
     "shell.execute_reply": "2021-01-27T23:35:03.377227Z"
    },
    "papermill": {
     "duration": 0.046535,
     "end_time": "2021-01-27T23:35:03.377719",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.331184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def translate_sentence_transformer(model, sentence, max_length=50):\n",
    "    model.eval()\n",
    "    tokens = [token.text.lower() for token in spacy_in_lang(sentence)]\n",
    "\n",
    "    text_to_indices = [in_lang.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    preds = [out_lang.vocab.stoi[out_lang.init_token]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        emb_src = model.encoder(sentence_tensor)\n",
    "        emb_src = model.pos_encoder(emb_src)\n",
    "\n",
    "        memory = model.transformer_encoder(emb_src)\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            trg = torch.Tensor(preds).long().unsqueeze(1).to(device)\n",
    "            trg = model.decoder(trg)\n",
    "            trg = model.pos_decoder(trg)\n",
    "\n",
    "            out = model.transformer_decoder(tgt = trg, memory = memory)\n",
    "            out = model.fc_out(out)\n",
    "            \n",
    "            \n",
    "\n",
    "            new = out.squeeze(1)[-1].argmax().item()\n",
    "            preds.append(new)\n",
    "            if new == out_lang.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "    \n",
    "    return \" \".join([out_lang.vocab.itos[i] for i in preds][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.448839Z",
     "iopub.status.busy": "2021-01-27T23:35:03.448198Z",
     "iopub.status.idle": "2021-01-27T23:35:03.451309Z",
     "shell.execute_reply": "2021-01-27T23:35:03.450777Z"
    },
    "papermill": {
     "duration": 0.041286,
     "end_time": "2021-01-27T23:35:03.451393",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.410107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_out_encoder(src):\n",
    "    \n",
    "    model.eval()\n",
    "    tokens = [token.text.lower() for token in spacy_in_lang(src)]\n",
    "\n",
    "    text_to_indices = [in_lang.vocab.stoi[token] for token in tokens]\n",
    "\n",
    "    # Convert to Tensor\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        emb_src = model.encoder(sentence_tensor)\n",
    "        emb_src = model.pos_encoder(emb_src)\n",
    "\n",
    "        memory = model.transformer_encoder(emb_src)\n",
    "\n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.533419Z",
     "iopub.status.busy": "2021-01-27T23:35:03.532655Z",
     "iopub.status.idle": "2021-01-27T23:35:03.535391Z",
     "shell.execute_reply": "2021-01-27T23:35:03.534958Z"
    },
    "papermill": {
     "duration": 0.05215,
     "end_time": "2021-01-27T23:35:03.535478",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.483328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def beam(phrase, k):\n",
    "    \n",
    "    model.eval()\n",
    "    memory = get_out_encoder(phrase)\n",
    "\n",
    "    sos = out_lang.vocab.stoi[\"<sos>\"]\n",
    "    tgt = [sos]\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        trg = torch.Tensor(tgt).long().unsqueeze(1).to(device)\n",
    "        trg = model.decoder(trg)\n",
    "        trg = model.pos_decoder(trg)\n",
    "\n",
    "        out = model.transformer_decoder(tgt = trg, memory = memory)\n",
    "        out = F.softmax(model.fc_out(out), dim=-1)[-1].squeeze()\n",
    "\n",
    "        args = out.argsort()[-k:].detach().cpu().numpy()\n",
    "        probs = out[args].detach().cpu().numpy()\n",
    "\n",
    "        probs = np.log(probs)\n",
    "        possible = list(zip([tgt + [args[i]] for i in range(k)], probs))\n",
    "        \n",
    "        for i in range(maxlen):\n",
    "\n",
    "            test=  []\n",
    "            for j in range(k):\n",
    "\n",
    "                tmp_tgt, tmp_prob = possible[j]\n",
    "\n",
    "                if tmp_tgt[-1] == out_lang.vocab.stoi[\"<eos>\"]:\n",
    "                    test.append(possible[j])\n",
    "\n",
    "                else:\n",
    "                    trg = torch.Tensor(tmp_tgt).long().unsqueeze(1).to(device)\n",
    "                    trg = model.decoder(trg)\n",
    "                    trg = model.pos_decoder(trg)\n",
    "\n",
    "                    out = model.transformer_decoder(tgt = trg, memory = memory)\n",
    "                    out = F.softmax(model.fc_out(out), dim=-1)[-1].squeeze()\n",
    "\n",
    "                    tmp_args = out.argsort()[-k:].detach().cpu().numpy()\n",
    "                    tmp_probs = out[tmp_args].detach().cpu().numpy()\n",
    "                    tmp_probs = (tmp_prob + np.log(tmp_probs))/(len(tmp_tgt)-1)\n",
    "\n",
    "                    for r in range(k): \n",
    "                        test.append((tmp_tgt + [tmp_args[r]], tmp_probs[r]))\n",
    "\n",
    "\n",
    "            possible = sorted(test, key=lambda x:x[1], reverse=True)[:k]\n",
    "            \n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.605209Z",
     "iopub.status.busy": "2021-01-27T23:35:03.604551Z",
     "iopub.status.idle": "2021-01-27T23:35:03.606903Z",
     "shell.execute_reply": "2021-01-27T23:35:03.607433Z"
    },
    "papermill": {
     "duration": 0.039291,
     "end_time": "2021-01-27T23:35:03.607532",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.568241",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert(x):\n",
    "    \n",
    "    sentence = x[0]\n",
    "    sentence = [out_lang.vocab.itos[i] for i in sentence]\n",
    "    \n",
    "    return (\" \".join(sentence), x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.681557Z",
     "iopub.status.busy": "2021-01-27T23:35:03.680881Z",
     "iopub.status.idle": "2021-01-27T23:35:03.683894Z",
     "shell.execute_reply": "2021-01-27T23:35:03.683485Z"
    },
    "papermill": {
     "duration": 0.044528,
     "end_time": "2021-01-27T23:35:03.683979",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.639451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_epoch(iterator, log_every=100):\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    n_tokens = 0\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    for batch_idx, batch in enumerate(iterator):\n",
    "        \n",
    "        inp_data = batch.src\n",
    "        target = batch.trg\n",
    "\n",
    "        output = model(inp_data, target)\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "\n",
    "        optimizer.optimizer.zero_grad()\n",
    "        loss = criterion(output, batch.trg_y)\n",
    "        total_loss += loss\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        n_tokens += batch.ntokens\n",
    "        if (batch_idx % log_every == 0) and (batch_idx > 0):\n",
    "            tokens_per_sec = n_tokens/(time.time() - start)\n",
    "            print(\" Step %d - Loss %f - Tokens per Sec %f\" % (batch_idx, loss.item(), tokens_per_sec))\n",
    "            \n",
    "        \n",
    "    return total_loss/len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.756992Z",
     "iopub.status.busy": "2021-01-27T23:35:03.756246Z",
     "iopub.status.idle": "2021-01-27T23:35:03.758521Z",
     "shell.execute_reply": "2021-01-27T23:35:03.759061Z"
    },
    "papermill": {
     "duration": 0.042876,
     "end_time": "2021-01-27T23:35:03.759185",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.716309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_validation(iterator, log_every=100):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, batch in enumerate(iterator):\n",
    "\n",
    "            inp_data = batch.src.to(device)\n",
    "            target = batch.trg.to(device)\n",
    "\n",
    "            output = model(inp_data, target[:-1, ])\n",
    "            output = output.reshape(-1, output.shape[2])\n",
    "            target = target[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss\n",
    "\n",
    "\n",
    "    return total_loss/len(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:35:03.856349Z",
     "iopub.status.busy": "2021-01-27T23:35:03.855566Z",
     "iopub.status.idle": "2021-01-27T23:54:45.443418Z",
     "shell.execute_reply": "2021-01-27T23:54:45.443890Z"
    },
    "papermill": {
     "duration": 1181.6524,
     "end_time": "2021-01-27T23:54:45.444053",
     "exception": false,
     "start_time": "2021-01-27T23:35:03.791653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 9.263897 - Tokens per Sec 47121.283573\n",
      " Step 200 - Loss 7.610021 - Tokens per Sec 50239.160757\n",
      " Step 300 - Loss 5.980570 - Tokens per Sec 50995.913070\n",
      " Step 400 - Loss 5.920236 - Tokens per Sec 52100.895727\n",
      " Step 500 - Loss 5.591289 - Tokens per Sec 52586.715767\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: so you can you to be the world .\n",
      "Expected: and when you do that , the ground around you starts to shift just a little bit .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i 'm a lot .\n",
      "Expected: so i had to find a way of solving this problem .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: it 's a lot of the world .\n",
      "Expected: disability is as visual as race .\n",
      "\n",
      "\n",
      " Train loss 6.9769978523254395 | Validation loss 4.979701995849609 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [1 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 4.633193 - Tokens per Sec 51526.444654\n",
      " Step 200 - Loss 5.227591 - Tokens per Sec 53038.572382\n",
      " Step 300 - Loss 4.168788 - Tokens per Sec 53555.258197\n",
      " Step 400 - Loss 3.819081 - Tokens per Sec 53870.821128\n",
      " Step 500 - Loss 4.937312 - Tokens per Sec 53974.571594\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: i 'm going to be the world .\n",
      "Expected: i got myself to the side of the lake .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i 've got to the same thing and the same .\n",
      "Expected: also , i realized there was a wide gap between north and south .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: and the first , a very important , very important , very important , very much .\n",
      "Expected: and the people who were highly trusted 20 years ago are still rather highly trusted : judges , nurses .\n",
      "\n",
      "\n",
      " Train loss 4.660784721374512 | Validation loss 4.12928581237793 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [2 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 4.448086 - Tokens per Sec 50931.952585\n",
      " Step 200 - Loss 4.629672 - Tokens per Sec 52848.218032\n",
      " Step 300 - Loss 4.174993 - Tokens per Sec 53305.035006\n",
      " Step 400 - Loss 3.988843 - Tokens per Sec 53618.430844\n",
      " Step 500 - Loss 4.307057 - Tokens per Sec 53795.294537\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: they 're going to be able to go to the same thing .\n",
      "Expected: he 's swimming down every avenue until he finally gets to the platform .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: so i 'm going to show you to show you what i 'm going to do .\n",
      "Expected: so i am going to give you some -- attempt today to try and explain to you what i do .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: so <unk> <unk> his name , and it 's called the <unk> and the stories of the stories of the <unk> .\n",
      "Expected: so lestrade needs his help , resents him , and sort of seethes with bitterness over the course of the mysteries .\n",
      "\n",
      "\n",
      " Train loss 4.108160018920898 | Validation loss 3.6690280437469482 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [3 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 3.800366 - Tokens per Sec 52418.855219\n",
      " Step 200 - Loss 3.884995 - Tokens per Sec 53434.045172\n",
      " Step 300 - Loss 3.610181 - Tokens per Sec 53829.890702\n",
      " Step 400 - Loss 3.872585 - Tokens per Sec 54045.680523\n",
      " Step 500 - Loss 3.632192 - Tokens per Sec 54176.620219\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: we 're going to be there .\n",
      "Expected: we 're born to make a difference .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i was all my hope .\n",
      "Expected: i lost all hope .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: i 'm not sure what i 'm going to do .\n",
      "Expected: i 'm not really sure about the answer .\n",
      "\n",
      "\n",
      " Train loss 3.7026734352111816 | Validation loss 3.3043181896209717 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [4 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 3.382319 - Tokens per Sec 52286.264874\n",
      " Step 200 - Loss 3.309083 - Tokens per Sec 53466.068797\n",
      " Step 300 - Loss 3.405709 - Tokens per Sec 53656.899066\n",
      " Step 400 - Loss 3.468428 - Tokens per Sec 53853.890558\n",
      " Step 500 - Loss 3.188089 - Tokens per Sec 54092.743326\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: i 'm going to go back and asked me , \" we 're going to buy this ? \"\n",
      "Expected: i turn to my wife and said , \" who are we buying this for ? \"\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i thought i was working for the age , and good people are good work in africa .\n",
      "Expected: i thought , age 21 , that we italians were good people and we were doing good work in africa .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: he 's a <unk> .\n",
      "Expected: he started a motorbike company .\n",
      "\n",
      "\n",
      " Train loss 3.3773350715637207 | Validation loss 3.0582244396209717 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [5 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 3.235371 - Tokens per Sec 52244.382269\n",
      " Step 200 - Loss 3.569374 - Tokens per Sec 52936.527042\n",
      " Step 300 - Loss 3.135065 - Tokens per Sec 53470.318612\n",
      " Step 400 - Loss 3.203926 - Tokens per Sec 53917.974414\n",
      " Step 500 - Loss 3.020876 - Tokens per Sec 53984.322517\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: all of them were just going to get it because we knew that we were doing it .\n",
      "Expected: now , everyone else had assumed it was , and we knew it was because we were working with it .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: they would be surprised what the point of the things that they can use it as well as the back .\n",
      "Expected: you 'd be surprised what the soil could do if you let it be your canvas .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: for us is the sun of the source of the source of the source of the world .\n",
      "Expected: for us , the sun is the source of prosperity .\n",
      "\n",
      "\n",
      " Train loss 3.1226980686187744 | Validation loss 2.9041037559509277 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [6 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.887235 - Tokens per Sec 51433.593085\n",
      " Step 200 - Loss 3.219445 - Tokens per Sec 53234.020756\n",
      " Step 300 - Loss 2.578462 - Tokens per Sec 53651.023875\n",
      " Step 400 - Loss 2.720620 - Tokens per Sec 53850.261065\n",
      " Step 500 - Loss 3.172727 - Tokens per Sec 53775.711370\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and so , it 's from rhythm and the rhythm , and the balance and if it falls .\n",
      "Expected: hup . now , it 's dependent on rhythm , and keeping a center of balance . when it falls , going underneath .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i had big lucky in a family , in education , in education and hundreds of women were withheld .\n",
      "Expected: i was very lucky to grow up in a family where education was prized and daughters were treasured .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: this <unk> is a great idea of a great idea of the retina of the retina .\n",
      "Expected: so growth mindset is a great idea for building grit .\n",
      "\n",
      "\n",
      " Train loss 2.9203381538391113 | Validation loss 2.811875820159912 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [7 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.307138 - Tokens per Sec 51583.456791\n",
      " Step 200 - Loss 2.105193 - Tokens per Sec 53196.889680\n",
      " Step 300 - Loss 2.887279 - Tokens per Sec 53610.614434\n",
      " Step 400 - Loss 2.815867 - Tokens per Sec 53596.516928\n",
      " Step 500 - Loss 2.394850 - Tokens per Sec 53822.491557\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: what would you do the same conditions ?\n",
      "Expected: what would they do under the same conditions ?\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: but you have to fight for the <unk> today .\n",
      "Expected: but you have to fight for your self - determination today .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: and his story is always changing the world around the world around the world .\n",
      "Expected: and his story repeats itself in urban centers around the world .\n",
      "\n",
      "\n",
      " Train loss 2.7503957748413086 | Validation loss 2.725186824798584 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [8 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.578315 - Tokens per Sec 53057.978445\n",
      " Step 200 - Loss 2.041968 - Tokens per Sec 53771.122345\n",
      " Step 300 - Loss 2.213793 - Tokens per Sec 53718.180888\n",
      " Step 400 - Loss 2.148998 - Tokens per Sec 54041.260391\n",
      " Step 500 - Loss 1.905277 - Tokens per Sec 54102.179587\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: now , who else would be in the house ?\n",
      "Expected: well , who else would be in the house ?\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: but if we do n't change the composition of the world we never do that .\n",
      "Expected: but if we do n't change the composition of the soil , we will never do this .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: and i said , \" i 'm not even saying , but maybe they 're the next .\n",
      "Expected: and then my supervisor said , \" not yet , but she might marry the next one .\n",
      "\n",
      "\n",
      " Train loss 2.593245506286621 | Validation loss 2.682738780975342 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [9 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.853822 - Tokens per Sec 51764.123595\n",
      " Step 200 - Loss 2.450645 - Tokens per Sec 52420.995542\n",
      " Step 300 - Loss 2.272076 - Tokens per Sec 53261.723634\n",
      " Step 400 - Loss 2.776500 - Tokens per Sec 53612.659855\n",
      " Step 500 - Loss 3.127543 - Tokens per Sec 54015.680771\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: how do we rank after the trust , how would you use the exploits to receive ?\n",
      "Expected: how do we mimic the way trust is built face - to - face online ?\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: the first principle of all , is respect .\n",
      "Expected: the first principle of aid is respect .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: it would be the world that 's the world that everybody can do . \"\n",
      "Expected: it will prove to the world that anybody can do this . \"\n",
      "\n",
      "\n",
      " Train loss 2.4720654487609863 | Validation loss 2.6463279724121094 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [10 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 0.971080 - Tokens per Sec 51931.512276\n",
      " Step 200 - Loss 2.620169 - Tokens per Sec 53094.729066\n",
      " Step 300 - Loss 2.453615 - Tokens per Sec 53490.737384\n",
      " Step 400 - Loss 1.819777 - Tokens per Sec 53802.352611\n",
      " Step 500 - Loss 2.725393 - Tokens per Sec 54074.651617\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: you can understand it .\n",
      "Expected: you can understand it .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: you can just read your book .\n",
      "Expected: just go and read her book .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: because they 're innocent , and everyone knows .\n",
      "Expected: because she 's blameless ; everybody knows it .\n",
      "\n",
      "\n",
      " Train loss 2.3784162998199463 | Validation loss 2.624661922454834 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [11 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.169956 - Tokens per Sec 51073.260638\n",
      " Step 200 - Loss 2.036286 - Tokens per Sec 52682.208107\n",
      " Step 300 - Loss 1.893907 - Tokens per Sec 53508.802330\n",
      " Step 400 - Loss 2.741091 - Tokens per Sec 53834.240291\n",
      " Step 500 - Loss 2.268780 - Tokens per Sec 53781.864487\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: for the time , they 're going to be under the skin , you do n't have to be able to do it .\n",
      "Expected: right now , they are under the skin , but in the future , they wo n't have to be implanted .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: so what i want to do here is to do this sexy .\n",
      "Expected: so what i want to do here , we got ta make this sexy .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: it was a beautiful day .\n",
      "Expected: it was a beautiful day .\n",
      "\n",
      "\n",
      " Train loss 2.3009395599365234 | Validation loss 2.6018292903900146 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [12 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.251959 - Tokens per Sec 52192.897976\n",
      " Step 200 - Loss 2.693904 - Tokens per Sec 53542.978027\n",
      " Step 300 - Loss 2.654059 - Tokens per Sec 53812.570396\n",
      " Step 400 - Loss 2.310062 - Tokens per Sec 53695.257017\n",
      " Step 500 - Loss 2.666543 - Tokens per Sec 53929.310969\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: but this voice said , \" come on , come on . \"\n",
      "Expected: but this voice kept calling me : \" come on , stay with me . \"\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: thank you .\n",
      "Expected: thank you .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: then we should trust : we should trust anymore .\n",
      "Expected: the second is an aim : we should have more trust .\n",
      "\n",
      "\n",
      " Train loss 2.2380211353302 | Validation loss 2.585987091064453 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [13 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.662900 - Tokens per Sec 53007.774756\n",
      " Step 200 - Loss 2.567327 - Tokens per Sec 53720.390021\n",
      " Step 300 - Loss 2.427064 - Tokens per Sec 53572.764960\n",
      " Step 400 - Loss 2.740786 - Tokens per Sec 53897.942105\n",
      " Step 500 - Loss 2.778171 - Tokens per Sec 53981.760247\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and we stitch them together , like medieval monks .\n",
      "Expected: and we stitch them together like medieval monks .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i want you to take it , but i want you to take your health . \"\n",
      "Expected: i want them to take it , but at the same time , i want them to take back their health . \"\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: well , the name is baker , you ca n't .\n",
      "Expected: well the name baker does n't actually mean anything to you .\n",
      "\n",
      "\n",
      " Train loss 2.1819069385528564 | Validation loss 2.591386318206787 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [14 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.399647 - Tokens per Sec 52484.415359\n",
      " Step 200 - Loss 2.427941 - Tokens per Sec 53418.802042\n",
      " Step 300 - Loss 2.093399 - Tokens per Sec 53495.278100\n",
      " Step 400 - Loss 1.822689 - Tokens per Sec 53806.516633\n",
      " Step 500 - Loss 2.460678 - Tokens per Sec 54152.388297\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: of course , there was a couple of places , but slower than slower .\n",
      "Expected: of course there were a few changes , but tools changed slower than skeletons in those days .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: we could n't believe it and said , \" look , \" look , how simple is . \"\n",
      "Expected: and we could not believe , and we were telling the zambians , \" look how easy agriculture is . \"\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: you have a mouth , in the door , it means , \" questions . \"\n",
      "Expected: put a mouth inside the door , asking questions .\n",
      "\n",
      "\n",
      " Train loss 2.1334187984466553 | Validation loss 2.574079990386963 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [15 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.581115 - Tokens per Sec 52837.980165\n",
      " Step 200 - Loss 2.047696 - Tokens per Sec 52716.838901\n",
      " Step 300 - Loss 1.874424 - Tokens per Sec 53242.517016\n",
      " Step 400 - Loss 1.873505 - Tokens per Sec 53582.188075\n",
      " Step 500 - Loss 2.357852 - Tokens per Sec 53849.363653\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: so , water , 400 miles , 400 kilometers , soon , it looks like this .\n",
      "Expected: so , water from 300 , 400 kilometers away , soon it become like this .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: there 's a mystery to work with entrepreneurs .\n",
      "Expected: however , there is a secret to work with entrepreneurs .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: we have the <unk> of gravity , and control movement , by the way , and the <unk> and <unk> .\n",
      "Expected: what we did was essentially canceling gravity and controlling the movement by combining magnetic levitation and mechanical actuation and sensing technologies .\n",
      "\n",
      "\n",
      " Train loss 2.0911872386932373 | Validation loss 2.560502052307129 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [16 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.979499 - Tokens per Sec 51375.823255\n",
      " Step 200 - Loss 2.325004 - Tokens per Sec 52917.573212\n",
      " Step 300 - Loss 2.228242 - Tokens per Sec 53661.966199\n",
      " Step 400 - Loss 2.421724 - Tokens per Sec 53879.653120\n",
      " Step 500 - Loss 1.688541 - Tokens per Sec 53819.100586\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: children with cleft palates were labeled as a leader , and to live under conditions .\n",
      "Expected: children with birth defects were labeled incurables , and confined for life to inhuman conditions .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: he did n't have more size than the words \" or \" or \"\n",
      "Expected: and it did n't have much more grandeur than the term \" foreman \" or \" overseer . \"\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: i recommend him to all of you . he 's in the internet .\n",
      "Expected: i recommend it to all of you . it 's on demand now .\n",
      "\n",
      "\n",
      " Train loss 2.0513975620269775 | Validation loss 2.550236701965332 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [17 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.831066 - Tokens per Sec 51876.611935\n",
      " Step 200 - Loss 2.260065 - Tokens per Sec 53382.184352\n",
      " Step 300 - Loss 2.307978 - Tokens per Sec 53761.203454\n",
      " Step 400 - Loss 2.350902 - Tokens per Sec 53831.419529\n",
      " Step 500 - Loss 2.495054 - Tokens per Sec 54009.915311\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: and a hammer , if we grab a hammer , that 's what we 're grabbing .\n",
      "Expected: and so a hammer , when we grab a hammer , that 's what we 're grabbing .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: when i was 11 years old , i was a racist by the morning , i was sealed .\n",
      "Expected: when i was 11 , i remember waking up one morning to the sound of joy in my house .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: careful in three women : \" women 's called \" <unk> . \"\n",
      "Expected: three women together , be careful , it 's adultery .\n",
      "\n",
      "\n",
      " Train loss 2.017760992050171 | Validation loss 2.54587984085083 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [18 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.295521 - Tokens per Sec 51642.405627\n",
      " Step 200 - Loss 2.506056 - Tokens per Sec 53240.689895\n",
      " Step 300 - Loss 1.543461 - Tokens per Sec 53388.864121\n",
      " Step 400 - Loss 1.816805 - Tokens per Sec 53791.476253\n",
      " Step 500 - Loss 2.394835 - Tokens per Sec 54005.807094\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: we 're just talking about one side .\n",
      "Expected: we only talk about one side of it .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: and i think there 's a lot of the national park , just so few lions .\n",
      "Expected: and i think this is why the nairobi national park lions are few .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: so i stayed one day something longer in school , much longer , and i held myself in the school district .\n",
      "Expected: so one day i stayed a little late after school , a little too late , and i lurked in the girls ' bathroom .\n",
      "\n",
      "\n",
      " Train loss 1.9842758178710938 | Validation loss 2.5546233654022217 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [19 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.757755 - Tokens per Sec 51952.134338\n",
      " Step 200 - Loss 2.324418 - Tokens per Sec 52576.662559\n",
      " Step 300 - Loss 1.992105 - Tokens per Sec 53229.245027\n",
      " Step 400 - Loss 1.503966 - Tokens per Sec 53513.358956\n",
      " Step 500 - Loss 1.710761 - Tokens per Sec 53805.830802\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: here 's a <unk> for <unk> .\n",
      "Expected: here 's a step - by - step that shows grinding hamburger .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: it was about the gas , for a long time .\n",
      "Expected: and this was debated in congress for ages and ages .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: like , books always with them .\n",
      "Expected: so may books be always with you .\n",
      "\n",
      "\n",
      " Train loss 1.9545789957046509 | Validation loss 2.549611806869507 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [20 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.397650 - Tokens per Sec 51627.083837\n",
      " Step 200 - Loss 1.680676 - Tokens per Sec 53223.622404\n",
      " Step 300 - Loss 1.988970 - Tokens per Sec 53725.835752\n",
      " Step 400 - Loss 2.409703 - Tokens per Sec 53962.632721\n",
      " Step 500 - Loss 2.356650 - Tokens per Sec 54004.258178\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: in other words , regret requires two things .\n",
      "Expected: so in other words , regret requires two things .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: these are the guys who helped me to have the truck that truck .\n",
      "Expected: these are the guys , they helped me unload the truck .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: they just perfect people .\n",
      "Expected: they only hired perfect people .\n",
      "\n",
      "\n",
      " Train loss 1.928989291191101 | Validation loss 2.553072452545166 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [21 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.285996 - Tokens per Sec 52557.634345\n",
      " Step 200 - Loss 1.913103 - Tokens per Sec 53597.787538\n",
      " Step 300 - Loss 1.972696 - Tokens per Sec 53932.171799\n",
      " Step 400 - Loss 2.338923 - Tokens per Sec 54184.443076\n",
      " Step 500 - Loss 1.987736 - Tokens per Sec 54068.493935\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: what if we tried to go to the brain into the brain to see where all this happens ?\n",
      "Expected: so what we 're trying to do is bore into the brain to see where all this happens .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: my mother got my mother , but she was born .\n",
      "Expected: but my educated mother became a teacher .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: the reason for my incarceration for this month , was actually the editorial cartoonist of a primal international leaders .\n",
      "Expected: my reason for being in mogadishu that month was actually to host a youth leadership and entrepreneurship summit .\n",
      "\n",
      "\n",
      " Train loss 1.9025810956954956 | Validation loss 2.5547187328338623 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [22 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.080581 - Tokens per Sec 52983.716463\n",
      " Step 200 - Loss 1.382492 - Tokens per Sec 53575.318895\n",
      " Step 300 - Loss 2.217402 - Tokens per Sec 53643.950976\n",
      " Step 400 - Loss 1.446657 - Tokens per Sec 53641.087811\n",
      " Step 500 - Loss 1.583246 - Tokens per Sec 53832.771087\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: for his services , he 's mostly with a cows or goats .\n",
      "Expected: he 's most often paid for his services in cash , cows or goats .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: it 's my favorite way to say you should be able to grow their own food .\n",
      "Expected: it 's my gospel , when i 'm telling people , grow your own food .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: people with whom i grew up with my friends , no problem with my faith .\n",
      "Expected: the people that i grew up with had no problem with my faith .\n",
      "\n",
      "\n",
      " Train loss 1.8813424110412598 | Validation loss 2.5531978607177734 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [23 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.093983 - Tokens per Sec 52416.135913\n",
      " Step 200 - Loss 1.843989 - Tokens per Sec 53150.187789\n",
      " Step 300 - Loss 1.914046 - Tokens per Sec 53368.907411\n",
      " Step 400 - Loss 2.368079 - Tokens per Sec 53681.225930\n",
      " Step 500 - Loss 1.693409 - Tokens per Sec 53922.181775\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: i think some of them are awesome , prayer .\n",
      "Expected: i love some of these , physical exercise , prayer .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: but these great conversations do n't come to mind if our scientists and engineers do n't invest in your own .\n",
      "Expected: but these great conversations ca n't occur if our scientists and engineers do n't invite us in to see their wonderland .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: and i knew i was soon to say \" <unk> \" <unk> \"\n",
      "Expected: and i knew that i would be promoted from \" diner diner \" to \" wacky best friend \" in no time .\n",
      "\n",
      "\n",
      " Train loss 1.8576823472976685 | Validation loss 2.5564661026000977 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [24 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.300681 - Tokens per Sec 50971.665806\n",
      " Step 200 - Loss 2.297247 - Tokens per Sec 52548.081695\n",
      " Step 300 - Loss 2.178472 - Tokens per Sec 53158.529765\n",
      " Step 400 - Loss 2.365478 - Tokens per Sec 53665.475989\n",
      " Step 500 - Loss 1.398213 - Tokens per Sec 53759.475965\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: you can freeze half the brain , it regrows .\n",
      "Expected: you can freeze half the brain . it regrows .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: it costs 300 dollars , by the way , the <unk> of the flow .\n",
      "Expected: takes about $ 300 , by the way , in the neurologist 's clinic to do it .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: sometimes the usual problems that technically solvable , but not as well .\n",
      "Expected: sometimes , big problems that had seemed technological turn out not to be so .\n",
      "\n",
      "\n",
      " Train loss 1.8375043869018555 | Validation loss 2.5521774291992188 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [25 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.328423 - Tokens per Sec 51563.318515\n",
      " Step 200 - Loss 1.343892 - Tokens per Sec 52736.447508\n",
      " Step 300 - Loss 1.818404 - Tokens per Sec 53534.021317\n",
      " Step 400 - Loss 2.275196 - Tokens per Sec 53856.688535\n",
      " Step 500 - Loss 1.407874 - Tokens per Sec 53575.687336\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: chris anderson : chris , i 've got a question .\n",
      "Expected: chris anderson : chris , i 've got a question for you .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: and i sent me a switch off the lights and the lights and neutralize .\n",
      "Expected: and i got a switch where i can switch on the lights , on and off .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: and i said , \" well , but maybe , but maybe they 're the next one .\n",
      "Expected: and then my supervisor said , \" not yet , but she might marry the next one .\n",
      "\n",
      "\n",
      " Train loss 1.8173167705535889 | Validation loss 2.5430331230163574 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [26 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.238660 - Tokens per Sec 52632.540498\n",
      " Step 200 - Loss 1.720098 - Tokens per Sec 53640.365846\n",
      " Step 300 - Loss 1.255776 - Tokens per Sec 53928.100923\n",
      " Step 400 - Loss 1.700388 - Tokens per Sec 53884.250866\n",
      " Step 500 - Loss 2.261194 - Tokens per Sec 54120.535006\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: so i decided to write a little bit of writing .\n",
      "Expected: so i decided to try and write something more fun .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: so scientists and engineers , engineers , if you have solved me , <unk> me .\n",
      "Expected: and so , scientists and engineers , when you 've solved this equation , by all means , talk nerdy to me .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: right .\n",
      "Expected: yeah .\n",
      "\n",
      "\n",
      " Train loss 1.798930287361145 | Validation loss 2.5497488975524902 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [27 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 2.073193 - Tokens per Sec 52900.283525\n",
      " Step 200 - Loss 2.180361 - Tokens per Sec 53299.567992\n",
      " Step 300 - Loss 1.594963 - Tokens per Sec 53202.414132\n",
      " Step 400 - Loss 1.514820 - Tokens per Sec 53452.658612\n",
      " Step 500 - Loss 1.378101 - Tokens per Sec 53666.601936\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: now , there 's about 60 percent of the big companies , it 's part of a limitation .\n",
      "Expected: by now , about 60 percent of the large companies it 's been part of the pension protection act .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: at the end , it ended up with you .\n",
      "Expected: so at the end , i had a settlement with them .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: but he was the second child and he was less <unk> less than his sister .\n",
      "Expected: but he was the second child , and the level of supervision had plummeted .\n",
      "\n",
      "\n",
      " Train loss 1.782762050628662 | Validation loss 2.553969621658325 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [28 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.749136 - Tokens per Sec 52431.786439\n",
      " Step 200 - Loss 2.053836 - Tokens per Sec 53313.455722\n",
      " Step 300 - Loss 2.208140 - Tokens per Sec 53545.366312\n",
      " Step 400 - Loss 1.499833 - Tokens per Sec 53642.452338\n",
      " Step 500 - Loss 1.356980 - Tokens per Sec 53953.325271\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: we say the world live in cities .\n",
      "Expected: you know , we say , half the world is living in cities .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: i want to share some sentences with you who wrote the wall .\n",
      "Expected: and i 'd like to share a few things that people wrote on this wall .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: \" oh , i ca n't believe that the <unk> has made a <unk> . \"\n",
      "Expected: \" aw , i ca n't believe pixar made a princess movie . \"\n",
      "\n",
      "\n",
      " Train loss 1.7660702466964722 | Validation loss 2.554868698120117 \n",
      "\n",
      "\n",
      "\n",
      "Epoch [29 / 30]\n",
      "\n",
      "\n",
      " Step 100 - Loss 1.653833 - Tokens per Sec 51720.048066\n",
      " Step 200 - Loss 1.974253 - Tokens per Sec 53013.335537\n",
      " Step 300 - Loss 1.763457 - Tokens per Sec 53483.893144\n",
      " Step 400 - Loss 2.148589 - Tokens per Sec 53891.175807\n",
      " Step 500 - Loss 2.075171 - Tokens per Sec 53969.350442\n",
      "\n",
      "Example #1 (from Train data): \n",
      "Translation: people hate number ten like 10 times as they hate uni .\n",
      "Expected: people hate number ten like they hate uni , actually .\n",
      "\n",
      "Example #2 (from Validation): \n",
      "Translation: and yeah , so all of these are being sold , but that 's just a little part of the <unk> .\n",
      "Expected: and yeah , they pretty much retouch all the photos , but that is only a small component of what 's happening .\n",
      "\n",
      "Example #3 (from Test data): \n",
      "Translation: so i looked at the <unk> and said , \" sorry , i can get another chair ? \"\n",
      "Expected: so i looked at the stage manager and i 'm like , \" excuse me , can i have another chair ? \"\n",
      "\n",
      "\n",
      " Train loss 1.7499794960021973 | Validation loss 2.5534255504608154 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = 6486468 \n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(f'Epoch [{epoch} / {num_epochs}]\\n')\n",
    "    \n",
    "    loss = run_epoch((rebatch(b) for b in train_iterator))\n",
    "    validation_loss = run_validation((rebatch(b) for b in valid_iterator))\n",
    "    \n",
    "    \n",
    "    rand_i01 = np.random.randint(0, len(train_data))\n",
    "    rand_i02 = np.random.randint(0, len(valid_data))\n",
    "    rand_i03 = np.random.randint(0, len(test_data))\n",
    "    \n",
    "    sentence01, expected01 = \" \".join(train_data[rand_i01].src), \" \".join(train_data[rand_i01].trg)\n",
    "    sentence02, expected02 = \" \".join(valid_data[rand_i02].src), \" \".join(valid_data[rand_i02].trg)\n",
    "    sentence03, expected03 = \" \".join(test_data[rand_i03].src), \" \".join(test_data[rand_i03].trg)\n",
    "\n",
    "    translated_sentence01 = translate_sentence_transformer(model, sentence01, max_length=50)\n",
    "    translated_sentence02 = translate_sentence_transformer(model, sentence02, max_length=50)\n",
    "    translated_sentence03 = translate_sentence_transformer(model, sentence03, max_length=50)\n",
    "    \n",
    "    print(f\"\\nExample #1 (from Train data): \\nTranslation: { translated_sentence01 }\\nExpected: { expected01 }\")\n",
    "    print(f\"\\nExample #2 (from Validation): \\nTranslation: { translated_sentence02 }\\nExpected: { expected02 }\")\n",
    "    print(f\"\\nExample #3 (from Test data): \\nTranslation: { translated_sentence03 }\\nExpected: { expected03 }\\n\")\n",
    "    \n",
    "    print(f\"\\n Train loss {loss} | Validation loss {validation_loss} \\n\\n\\n\")\n",
    "    \n",
    "    \n",
    "    if validation_loss < best_loss:\n",
    "        torch.save(model.state_dict(), \"../models/new_transformer\")\n",
    "        best_loss = validation_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-27T23:54:45.626344Z",
     "iopub.status.busy": "2021-01-27T23:54:45.625504Z",
     "iopub.status.idle": "2021-01-27T23:54:46.107899Z",
     "shell.execute_reply": "2021-01-27T23:54:46.107420Z"
    },
    "papermill": {
     "duration": 0.57533,
     "end_time": "2021-01-27T23:54:46.107998",
     "exception": false,
     "start_time": "2021-01-27T23:54:45.532668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5430, device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"../models/new_transformer\"))\n",
    "\n",
    "run_validation((rebatch(b) for b in valid_iterator))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "papermill": {
   "duration": 1317.983714,
   "end_time": "2021-01-27T23:54:47.388509",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-27T23:32:49.404795",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
