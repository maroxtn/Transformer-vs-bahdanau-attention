{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzDcfnOvPYnR"
   },
   "outputs": [],
   "source": [
    "#Make a config file in which you choose the dataset for training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k#IWSLT\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "xMOTOdRgHlAl",
    "outputId": "d9edc366-d533-4859-e2b8-88b2a4090258"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "BkhAPN502QtF",
    "outputId": "aa86a622-f122-4570-94e3-65c04871c452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "spacy_ger = de_core_news_sm.load()\n",
    "spacy_eng = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4xiGiEV4280s"
   },
   "outputs": [],
   "source": [
    "def tokenizer_de(text):\n",
    "  return [tok.text for tok in spacy_ger.tokenizer(text)]\n",
    "\n",
    "def tokenizer_eng(text):\n",
    "  return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cvgTWYkg46Zz",
    "outputId": "f6c81dc4-c3e3-418f-b14d-10a1810a8a2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ate', 'my', 'friends', \"'s\", \"O'neal\", 'apple', 'yesterday']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_eng(\"I ate my friends's O'neal apple yesterday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G-IYtkU05ACP"
   },
   "outputs": [],
   "source": [
    "german = Field(tokenize=tokenizer_de, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\")\n",
    "\n",
    "english = Field(\n",
    "    tokenize=tokenizer_eng, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tG2khqwP5RkZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading training.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\multi30k\\training.tar.gz: 100%|██████████████████████████████████████████████| 1.21M/1.21M [00:01<00:00, 669kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading validation.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\multi30k\\validation.tar.gz: 100%|████████████████████████████████████████████| 46.3k/46.3k [00:00<00:00, 216kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading mmt_task1_test2016.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".data\\multi30k\\mmt_task1_test2016.tar.gz: 100%|████████████████████████████████████| 66.2k/66.2k [00:00<00:00, 150kB/s]\n"
     ]
    }
   ],
   "source": [
    "def f(x):    \n",
    "    return len(vars(x)['src']) <= 50 and len(vars(x)['trg']) <= 50\n",
    "\n",
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "        exts=('.de', '.en'), fields=(german, english), \n",
    "        filter_pred=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zZofP4s77Zkl"
   },
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "english.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five most common words in the dataset: [('a', 49165), ('.', 27623), ('in', 14886), ('the', 10955), ('on', 8035)]\n"
     ]
    }
   ],
   "source": [
    "#vocab.freqs in a python counter datatype\n",
    "print(\"Five most common words in the dataset: \" + str(english.vocab.freqs.most_common(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RRksTdJk86iE"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, bidirectional=True)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.energy = nn.Linear(hidden_size * 3, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_length, N) where N is batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (seq_length, N, embedding_size)\n",
    "\n",
    "        encoder_states, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape: (seq_length, N, hidden_size*2)\n",
    "\n",
    "        # Use forward, backward cells and hidden through a linear layer\n",
    "        # so that it can be input to the decoder which is not bidirectional\n",
    "        # Also using index slicing ([idx:idx+1]) to keep the dimension\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        \n",
    "        \n",
    "\n",
    "        # encoder_states: (seq_length, N, hidden_size*2)\n",
    "        sequence_length = encoder_states.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        # h_reshaped: (seq_length, N, hidden_size)\n",
    "\n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        # energy: (seq_length, N, 1)\n",
    "\n",
    "        attention = self.softmax(energy)\n",
    "        # attention: (seq_length, N, 1)\n",
    "\n",
    "        # attention: (seq_length, N, 1), snk\n",
    "        # encoder_states: (seq_length, N, hidden_size*2), snl\n",
    "        # we want context_vector: (1, N, hidden_size*2), i.e knl\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return encoder_states, hidden, cell, context_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QrDzJNnC9BcE"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_size, embedding_size, hidden_size, output_size, num_layers, p\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(hidden_size * 2 + embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, encoder_states, hidden, cell, context_vector):\n",
    "        x = x.unsqueeze(0)\n",
    "        # x: (1, N) where N is the batch size\n",
    "\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape: (1, N, embedding_size)\n",
    "        \n",
    "\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        # rnn_input: (1, N, hidden_size*2 + embedding_size)\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "        # outputs shape: (1, N, hidden_size)\n",
    "\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "        # predictions: (N, hidden_size)\n",
    "\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9OYoA3waBGZF"
   },
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(english.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        encoder_states, hidden, cell, context_vector = self.encoder(source)\n",
    "\n",
    "        # First input will be <SOS> token\n",
    "        x = target[0]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            # At every time step use encoder_states and update hidden, cell\n",
    "            output, hidden, cell = self.decoder(x, encoder_states, hidden, cell, context_vector)\n",
    "\n",
    "            # Store prediction for current time step\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Get the best word the Decoder predicted (index in the vocabulary)\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            # Teacher Forcing is used so that the model gets used to seeing\n",
    "            # similar inputs at training and testing time, if teacher forcing is 1\n",
    "            # then inputs at test time might be completely different than what the\n",
    "            # network is used to. This was a long comment.\n",
    "            x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Training Hyperparameters\n",
    "num_epochs = 100\n",
    "lr = 3e-4\n",
    "batch_size = 256\n",
    "d_model = 256\n",
    "\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "\n",
    "encoder_embedding_size = d_model\n",
    "decoder_embedding_size = d_model\n",
    "hidden_size = d_model*6\n",
    "\n",
    "num_layers = 1\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, \n",
    "                      hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, \n",
    "                      hidden_size, output_size, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_pad_idx = english.vocab.stoi['<pad>']\n",
    "de_pad_idx = german.vocab.stoi['<pad>']\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "def run_epoch():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        output = model(inp_data, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        sys.stdout.write(\"\\r %d\" % (batch_idx))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    return total_loss / len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(valid_iterator):\n",
    "        \n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "\n",
    "        output = model(inp_data, target)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, german, english, device, max_length=50):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    tokens = [token.text.lower() for token in spacy_ger(sentence)]\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    preds = [english.vocab.stoi[english.init_token]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        encoder_states, hidden, cell, context_vector = model.encoder(sentence_tensor)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "                    \n",
    "            trg = torch.Tensor([preds[-1]]).long().to(device)\n",
    "\n",
    "            output, hidden, cell = model.decoder(trg, encoder_states, hidden, cell, context_vector)\n",
    "            new = output.argmax(1).item()\n",
    "            \n",
    "            preds.append(new)\n",
    "            \n",
    "            if new == english.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "        \n",
    "    return [english.vocab.itos[i] for i in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam(phrase, k):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sos = english.vocab.stoi[\"<sos>\"]\n",
    "    tgt = [sos]\n",
    "    \n",
    "    #Prepare sentence\n",
    "    tokens = [token.text.lower() for token in spacy_ger(phrase)]\n",
    "    tokens.append(german.eos_token)\n",
    "\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #Get encoder output\n",
    "        encoder_states, hidden, cell, context_vector = model.encoder(sentence_tensor)\n",
    "        \n",
    "        \n",
    "        #Get first output from model\n",
    "        trg = torch.Tensor(tgt).long().to(device)\n",
    "\n",
    "        output, hidden, cell = model.decoder(trg[0:1], encoder_states, hidden, cell, context_vector)\n",
    "        out = F.softmax(output).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        args = out.argsort()[-k:]\n",
    "        probs = out[args].detach().cpu().numpy()\n",
    "        \n",
    "        args = args.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        probs = np.log(probs)\n",
    "        possible = list(zip([tgt + [args[i]] for i in range(k)], probs, [hidden.clone() for j in range(k)], [cell.clone() for j in range(k)]))\n",
    "\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            test=  []\n",
    "            for j in range(k):\n",
    "\n",
    "                tmp_tgt, tmp_prob, tmp_hidden, tmp_cell = possible[j]\n",
    "\n",
    "                if tmp_tgt[-1] == english.vocab.stoi[\"<eos>\"]:  #If sentence already ended\n",
    "                    test.append(possible[j])\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    #Compute output\n",
    "                    trg = torch.Tensor(tmp_tgt).long().to(device)\n",
    "\n",
    "                    output, hidden, cell = model.decoder(trg[i:i+1], encoder_states, tmp_hidden, tmp_cell, context_vector)\n",
    "                    out = F.softmax(output).squeeze()\n",
    "                    \n",
    "                    \n",
    "                    tmp_args = out.argsort()[-k:]\n",
    "                    tmp_probs = out[args].detach().cpu().numpy()\n",
    "\n",
    "                    tmp_args = tmp_args.detach().cpu().numpy()\n",
    "                    tmp_probs = (tmp_prob + np.log(tmp_probs))/(len(tmp_tgt)-1)\n",
    "\n",
    "\n",
    "                    for r in range(k): \n",
    "                        test.append((tmp_tgt + [tmp_args[r]], tmp_probs[r], hidden, cell))\n",
    "\n",
    "\n",
    "            possible = sorted(test, key=lambda x:x[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "                    \n",
    "    \n",
    "    return possible\n",
    "\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    \n",
    "    sentence = x[0]\n",
    "    sentence = [english.vocab.itos[i] for i in sentence]\n",
    "    \n",
    "    return (\" \".join(sentence), x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence =  \"Da ich hungrig bin, möchte ich essen.\"\n",
    "best_loss = 65646\n",
    "\n",
    "for epoch in range(20):\n",
    "    \n",
    "    print(f'Epoch [{epoch} / {num_epochs}]')\n",
    "\n",
    "\n",
    "    loss =  run_epoch()\n",
    "    validation_loss = run_validation()\n",
    "    \n",
    "    translated_sentence = translate_sentence(model, sentence, german, english, device, max_length=50)\n",
    "    out = beam(sentence, 3) \n",
    "    \n",
    "    \n",
    "    print(f\"Translated example sentence: \\n {list(map(convert, out[:2]))}\")\n",
    "    print(f\"Greedy: {translated_sentence}\")\n",
    "    \n",
    "    print(f\"\\n Train loss {loss} | Validation loss {validation_loss} \\n \\n\")\n",
    "    \n",
    "    if validation_loss < best_loss:\n",
    "        torch.save(model, \"best_model\")\n",
    "        best_loss = validation_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam(phrase, k):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sos = english.vocab.stoi[\"<sos>\"]\n",
    "    tgt = [sos]\n",
    "    \n",
    "    #Prepare sentence\n",
    "    tokens = [token.text.lower() for token in spacy_ger(phrase)]\n",
    "    tokens.append(german.eos_token)\n",
    "    tokens.insert(0, german.init_token)\n",
    "\n",
    "    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #Get encoder output\n",
    "        encoder_states, hidden, cell = model.encoder(sentence_tensor)\n",
    "        \n",
    "        \n",
    "        #Get first output from model\n",
    "        trg = torch.Tensor([tgt[-1]]).long().to(device)\n",
    "\n",
    "        output, hidden, cell = model.decoder(trg, encoder_states, hidden, cell)\n",
    "        out = F.softmax(output).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        args = out.argsort()[-k:]\n",
    "        probs = out[args].detach().cpu().numpy()\n",
    "        \n",
    "        args = args.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        probs = np.log(probs)\n",
    "        possible = list(zip([tgt + [args[i]] for i in range(k)], probs, [hidden.clone() for j in range(k)], [cell.clone() for j in range(k)]))\n",
    "\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            test=  []\n",
    "            for j in range(k):\n",
    "\n",
    "                tmp_tgt, tmp_prob, tmp_hidden, tmp_cell = possible[j]\n",
    "\n",
    "                if tmp_tgt[-1] == english.vocab.stoi[\"<eos>\"]:  #If sentence already ended\n",
    "                    test.append(possible[j])\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    #Compute output\n",
    "                    trg = torch.Tensor([tmp_tgt[-1]]).long().to(device)\n",
    "\n",
    "                    output, hidden, cell = model.decoder(trg, encoder_states, tmp_hidden, tmp_cell)\n",
    "                    out = F.softmax(output).squeeze()\n",
    "                    \n",
    "                    \n",
    "                    tmp_args = out.argsort()[-k:]\n",
    "                    tmp_probs = out[args].detach().cpu().numpy()\n",
    "\n",
    "                    tmp_args = tmp_args.detach().cpu().numpy()\n",
    "                    tmp_probs = (tmp_prob + np.log(tmp_probs))/(len(tmp_tgt)-1)\n",
    "\n",
    "\n",
    "                    for r in range(k): \n",
    "                        test.append((tmp_tgt + [tmp_args[r]], tmp_probs[r], hidden, cell))\n",
    "\n",
    "\n",
    "            possible = sorted(test, key=lambda x:x[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "                    \n",
    "    \n",
    "    return possible\n",
    "\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    \n",
    "    sentence = x[0]\n",
    "    sentence = [english.vocab.itos[i] for i in sentence]\n",
    "    \n",
    "    return (\" \".join(sentence), x[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
