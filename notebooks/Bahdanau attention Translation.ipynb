{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FzDcfnOvPYnR"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k#IWSLT\n",
    "from torchtext.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "import sys \n",
    "\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\"IN_LANG\":\"en\", \"OUT_LANG\": \"de\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "BkhAPN502QtF",
    "outputId": "aa86a622-f122-4570-94e3-65c04871c452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "✔ Download and installation successful\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "\n",
    "\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "spacy.cli.download(\"de_core_news_sm\")\n",
    "\n",
    "\n",
    "if CFG[\"IN_LANG\"] == \"en\":\n",
    "    spacy_in_lang = en_core_web_sm.load()\n",
    "    spacy_out_lang = de_core_news_sm.load()\n",
    "else:\n",
    "    spacy_in_lang = de_core_news_sm.load()\n",
    "    spacy_out_lang = en_core_web_sm.load()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_in(text):\n",
    "    return [tok.text for tok in spacy_in_lang.tokenizer(text)]\n",
    "\n",
    "def tokenizer_out(text):\n",
    "    return [tok.text for tok in spacy_out_lang.tokenizer(text)]\n",
    "\n",
    "\n",
    "in_lang = Field(tokenize=tokenizer_in, lower=True, include_lengths=True)\n",
    "out_lang = Field(tokenize=tokenizer_out, lower=True, init_token=\"<sos>\", eos_token=\"<eos>\", include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(\n",
    "        exts=(\".\"+CFG[\"IN_LANG\"], \".\"+CFG[\"OUT_LANG\"]), fields=(in_lang, out_lang ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_lang.build_vocab(train_data, max_size=10000, min_freq=2)\n",
    "out_lang.build_vocab(train_data, max_size=10000, min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module): \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, bidirectional=True)\n",
    "\n",
    "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "\n",
    "    def forward(self, x, inp_length=None):\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        \n",
    "        if inp_length == None:\n",
    "            encoder_states, hidden = self.rnn(embedding)\n",
    "        else:      \n",
    "            packed = pack_padded_sequence(embedding, inp_length.cpu()) #To speed up training\n",
    "            encoder_states, hidden = self.rnn(packed)\n",
    "            encoder_states, _ = pad_packed_sequence(encoder_states)\n",
    "\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "\n",
    "        return encoder_states, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.GRU(hidden_size * 2 + embedding_size, hidden_size, num_layers)\n",
    "\n",
    "        self.energy = nn.Linear(hidden_size, 1)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.fc_key = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc_query = nn.Linear(hidden_size*2, hidden_size)\n",
    "\n",
    "    def forward(self, x, encoder_states, hidden, source):\n",
    "        \n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "\n",
    "        \n",
    "        key = self.fc_key(hidden)\n",
    "        query = self.fc_query(encoder_states)\n",
    "        \n",
    "        energy = key+query\n",
    "        energy = self.energy(torch.tanh(energy))\n",
    "        energy = energy.squeeze(-1).masked_fill_((source == in_pad_idx), -float('inf')).unsqueeze(-1)\n",
    "\n",
    "        attention = F.softmax(energy, dim=0)\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "\n",
    "        #Concatenate the context vector with the embedding of the previous word, and feed it to the GRU\n",
    "        rnn_input = torch.cat((context_vector, embedding), dim=2)\n",
    "        outputs, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "\n",
    "        return predictions, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, source, target, inp_length):\n",
    "        \n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = len(out_lang.vocab)\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        encoder_states, hidden = self.encoder(source, inp_length)\n",
    "\n",
    "        x = target[0] #<SOS>\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "\n",
    "            output, hidden = self.decoder(x, encoder_states, hidden, source)\n",
    "\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "\n",
    "            x = target[t] #No teacher forcing\n",
    "            \n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#Training Hyperparameters\n",
    "num_epochs = 100\n",
    "lr = 3e-4\n",
    "batch_size = 64\n",
    "d_model = 128\n",
    "\n",
    "input_size_encoder = len(in_lang.vocab)\n",
    "input_size_decoder = len(out_lang.vocab)\n",
    "output_size = len(out_lang.vocab)\n",
    "\n",
    "\n",
    "encoder_embedding_size = d_model\n",
    "decoder_embedding_size = d_model\n",
    "hidden_size = d_model*4\n",
    "\n",
    "num_layers = 1\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=batch_size,\n",
    "    sort_within_batch=True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, \n",
    "                      hidden_size, num_layers, dropout).to(device)\n",
    "\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, \n",
    "                      hidden_size, output_size, num_layers, dropout).to(device)\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pad_idx = in_lang.vocab.stoi['<pad>']\n",
    "out_pad_idx = out_lang.vocab.stoi['<pad>']\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=out_pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence_bahdanau(model, sentence, max_length=50): #Translate from raw text using the trained model\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    tokens = [token.text.lower() for token in spacy_in_lang(sentence)]\n",
    "\n",
    "    text_to_indices = [in_lang.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n",
    "\n",
    "    preds = [out_lang.vocab.stoi[out_lang.init_token]]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        encoder_states, hidden = model.encoder(sentence_tensor)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "                    \n",
    "            trg = torch.Tensor([preds[-1]]).long().to(device)\n",
    "\n",
    "            output, hidden = model.decoder(trg, encoder_states, hidden, sentence_tensor,print_att=True)\n",
    "            new = output.argmax(1).item()\n",
    "            \n",
    "            preds.append(new)\n",
    "            \n",
    "            if new == out_lang.vocab.stoi[\"<eos>\"]:\n",
    "                break\n",
    "            \n",
    "        \n",
    "    return [out_lang.vocab.itos[i] for i in preds][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam(phrase, k):  #K: beam width\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    sos = out_lang.vocab.stoi[\"<sos>\"]\n",
    "    tgt = [sos]\n",
    "    \n",
    "    #Prepare sentence\n",
    "    tokens = [token.text.lower() for token in spacy_in_lang(phrase)]\n",
    "    tokens.append(in_lang.eos_token)\n",
    "    tokens.insert(0, in_lang.init_token)\n",
    "\n",
    "    text_to_indices = [in_lang.vocab.stoi[token] for token in tokens]\n",
    "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)    \n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        #Get encoder output\n",
    "        encoder_states, hidden = model.encoder(sentence_tensor)\n",
    "        \n",
    "        \n",
    "        #Get first output from model\n",
    "        trg = torch.Tensor([tgt[-1]]).long().to(device)\n",
    "\n",
    "        output, hidden = model.decoder(trg, encoder_states, hidden,sentence_tensor)\n",
    "        out = F.softmax(output).squeeze()\n",
    "\n",
    "\n",
    "\n",
    "        args = out.argsort()[-k:]\n",
    "        probs = out[args].detach().cpu().numpy()\n",
    "        \n",
    "        args = args.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        probs = np.log(probs)\n",
    "        possible = list(zip([tgt + [args[i]] for i in range(k)], probs, [hidden.clone() for j in range(k)]))\n",
    "\n",
    "\n",
    "        for i in range(50):\n",
    "\n",
    "            test=  []\n",
    "            for j in range(k):\n",
    "\n",
    "                tmp_tgt, tmp_prob, tmp_hidden = possible[j]\n",
    "\n",
    "                if tmp_tgt[-1] == out_lang.vocab.stoi[\"<eos>\"]:  #If sentence already ended\n",
    "                    test.append(possible[j])\n",
    "\n",
    "                else:\n",
    "                    \n",
    "                    #Compute output\n",
    "                    trg = torch.Tensor([tmp_tgt[-1]]).long().to(device)\n",
    "\n",
    "                    output, hidden = model.decoder(trg, encoder_states, tmp_hidden, sentence_tensor)\n",
    "                    out = F.softmax(output).squeeze()\n",
    "                    \n",
    "                    \n",
    "                    tmp_args = out.argsort()[-k:]\n",
    "                    tmp_probs = out[args].detach().cpu().numpy()\n",
    "\n",
    "                    tmp_args = tmp_args.detach().cpu().numpy()\n",
    "                    tmp_probs = (tmp_prob + np.log(tmp_probs))/(len(tmp_tgt)-1)\n",
    "\n",
    "\n",
    "                    for r in range(k): \n",
    "                        test.append((tmp_tgt + [tmp_args[r]], tmp_probs[r], hidden))\n",
    "\n",
    "\n",
    "            possible = sorted(test, key=lambda x:x[1], reverse=True)[:k]\n",
    "\n",
    "\n",
    "                    \n",
    "    \n",
    "    return possible\n",
    "\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    \n",
    "    sentence = x[0]\n",
    "    sentence = [out_lang.vocab.itos[i] for i in sentence]\n",
    "    \n",
    "    return (\" \".join(sentence), x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        inp_data, inp_length = batch.src\n",
    "        inp_data = inp_data.to(device)\n",
    "        \n",
    "        target, target_length = batch.trg\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(inp_data, target, inp_length)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        sys.stdout.write(\"\\r %d\" % (batch_idx))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    return total_loss / len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch():\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_iterator):\n",
    "        \n",
    "        inp_data, inp_length = batch.src\n",
    "        inp_data = inp_data.to(device)\n",
    "        \n",
    "        target, target_length = batch.trg\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(inp_data, target, inp_length)\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        sys.stdout.write(\"\\r %d\" % (batch_idx))\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    return total_loss / len(train_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation():\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, batch in enumerate(valid_iterator):\n",
    "        \n",
    "        inp_data, inp_length = batch.src\n",
    "        inp_data = inp_data.to(device)\n",
    "        \n",
    "        target, target_length = batch.trg\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(inp_data, target, inp_length)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'a man in green holds a guitar while the other man observes his shirt .'\n",
    "\n",
    "best_loss = 65646"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(20):\n",
    "    \n",
    "    print(f'Epoch [{epoch} / {num_epochs}]')\n",
    "\n",
    "\n",
    "    loss =  run_epoch()\n",
    "    validation_loss = run_validation()\n",
    "    \n",
    "    translated_sentence = translate_sentence_bahdanau(model, sentence, max_length=50)\n",
    "    out = beam(sentence, 3) \n",
    "    \n",
    "    \n",
    "    print(f\"Translated example sentence: \\n {list(map(convert, out[:2]))}\")\n",
    "    print(f\"Greedy: {translated_sentence}\")\n",
    "    \n",
    "    print(f\"\\n Train loss {loss} | Validation loss {validation_loss} \\n \\n\")\n",
    "    \n",
    "    if validation_loss < best_loss:\n",
    "        torch.save(model.state_dict(), \"../models/rnn_model\")\n",
    "        best_loss = validation_loss"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Translation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
